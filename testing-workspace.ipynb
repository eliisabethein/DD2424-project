{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bd4b5a9610>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Mihaela\n",
      "[nltk_data]     Stoycheva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sentences: 42068\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = TweetTokenizer(preserve_case=False)\n",
    "tok = MWETokenizer([('<', 'unk', '>')], separator = '')\n",
    "\n",
    "train_data = []\n",
    "vocabulary = []\n",
    "\n",
    "with open(\"data/ptb.train.txt\") as f:\n",
    "    total_sentences = 0\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        s = word_tokenize(line)\n",
    "        s = tok.tokenize(s)\n",
    "        s = ['<sos>'] + s + ['<eos>']\n",
    "        total_sentences += 1\n",
    "        train_data.append(s)\n",
    "        vocabulary += s\n",
    "\n",
    "vocabulary = sorted(list(set(vocabulary)))\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "print(\"total sentences:\", total_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '$', '&', \"'\", \"'80s\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '.', '10-year', '100-share', '12-month', '12-year', '13-week', '13th', '14-year-old', '190-point', '190.58-point', '1920s', '1930s', '1950s', '1960s', '1970s', '1980s', '1990s', '19th', '1\\\\/2-year', '2-for-1', '20-year', '20th', '24-hour', '26-week', '30-day', '30-share', '30-year', '300-a-share', '300-day', '40-year-old', '45-year-old', '500-stock', '52-week', '<eos>', '<sos>', '<unk>', 'N', '\\\\*', '\\\\*\\\\*', 'a', 'a.', 'a.c.', 'a.g.', 'a.m', 'a.m.', 'a.p', 'ab', 'aba', 'abandon', 'abandoned', 'abandoning', 'abbie', 'abc', 'ability', 'able', 'abm', 'aboard', 'abolish', 'abolished', 'aborted', 'abortion', 'abortion-rights', 'abortions', 'about', 'above', 'abrams', 'abramson', 'abroad', 'abrupt', 'abruptly', 'absence', 'absolutely', 'absorb', 'absorbed', 'absurd', 'abundant', 'abuse', 'abused', 'abuses', 'academic', 'academy', 'acadia', 'accelerate', 'accelerated', 'accelerating', 'acceleration', 'accept', 'acceptable', 'acceptance']\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66888001, 97292600)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model1 = Word2Vec(train_data, min_count=1, size=100, window=5)\n",
    "model1.train(train_data, len(train_data), epochs=100, total_examples=model1.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shares', 0.6012330055236816),\n",
       " ('stocks', 0.56917405128479),\n",
       " ('equity', 0.49792689085006714),\n",
       " ('share', 0.49605488777160645),\n",
       " ('mercantile', 0.4488602876663208),\n",
       " ('junk', 0.44790762662887573),\n",
       " ('sterling', 0.42159849405288696),\n",
       " ('junk-bond', 0.4125289022922516),\n",
       " ('options', 0.4084639549255371),\n",
       " ('plunge', 0.4042600989341736)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar(\"stock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('f.', 0.49283674359321594),\n",
       " ('e.', 0.4552537202835083),\n",
       " ('h.', 0.44416117668151855),\n",
       " ('w.', 0.4380009174346924),\n",
       " ('s.', 0.3974824547767639),\n",
       " ('l.', 0.39728131890296936),\n",
       " ('p.', 0.3883320987224579),\n",
       " ('wash.', 0.384010374546051),\n",
       " ('fluor', 0.37740230560302734),\n",
       " ('n.j.', 0.3744466304779053)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loans', 0.4509538412094116),\n",
       " ('loan', 0.42039382457733154),\n",
       " ('loan-loss', 0.41511422395706177),\n",
       " ('financial', 0.4025123119354248),\n",
       " ('debt', 0.3996398150920868),\n",
       " ('allowance', 0.396798700094223),\n",
       " ('receivables', 0.39609915018081665),\n",
       " ('credit-card', 0.3795093595981598),\n",
       " ('home-equity', 0.3659972548484802),\n",
       " ('dai-ichi', 0.36121222376823425)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar(\"credit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8408158 ,  2.1528978 ,  1.577311  , -0.5424135 , -0.77825797,\n",
       "       -3.487012  , -4.1693244 ,  1.2087706 ,  2.6357746 , -1.2623004 ,\n",
       "        1.9656764 ,  3.674057  , -0.04240939, -0.88878816,  1.9305915 ,\n",
       "        0.50211966,  1.0581901 ,  1.0795053 , -0.74899954,  2.7184741 ,\n",
       "       -2.354455  , -2.7535293 ,  4.412339  ,  1.6338178 , -1.503997  ,\n",
       "       -1.8006854 ,  0.9367318 ,  5.7195153 , -3.2364767 ,  0.9345901 ,\n",
       "       -1.8378781 , -0.6263213 ,  2.9540033 ,  2.7205722 ,  1.0509466 ,\n",
       "        0.5784444 , -4.2308373 ,  3.083613  ,  0.39906096,  0.20975357,\n",
       "        0.84889466, -0.6421074 ,  0.50029224,  1.4838523 , -3.8019986 ,\n",
       "       -0.8341925 ,  1.4484494 ,  1.0791607 ,  2.0743005 ,  0.58189577,\n",
       "        1.509926  ,  0.1290245 , -1.772432  , -0.22600487,  0.52499324,\n",
       "       -0.19749175, -1.8292658 ,  1.5739354 ,  0.9197604 ,  2.1917448 ,\n",
       "       -0.4205204 , -0.8827047 , -1.0129049 ,  0.5440537 , -1.0531436 ,\n",
       "       -1.1322188 ,  3.1973696 , -4.4078317 ,  1.8610548 , -1.500866  ,\n",
       "       -2.3738346 , -0.7888005 ,  1.2666152 , -1.1959288 , -0.5033279 ,\n",
       "        1.2274218 , -5.809245  , -2.463357  , -1.6656173 ,  0.6692856 ,\n",
       "        0.8773261 , -2.5972576 ,  1.1471876 ,  0.72151804,  1.2014047 ,\n",
       "       -1.6323485 ,  0.8367807 , -0.13361624,  0.9417116 ,  1.603805  ,\n",
       "       -0.4694209 , -1.7844027 , -1.6139027 ,  2.4976084 , -0.47952598,\n",
       "        0.20343244,  0.11815906,  2.4012218 , -1.9104013 ,  2.1913192 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv['credit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(vocabulary)\n",
    "input_size = 100\n",
    "output_size = 100\n",
    "hidden_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word = 'money'\n",
    "embedding_vec = model1.wv[input_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the word embeddings into a pythorch tensor\n",
    "weights = torch.FloatTensor(model1.wv.vectors)\n",
    "\n",
    "# NN MODEL\n",
    "embedded = nn.Embedding.from_pretrained(weights) #input layer\n",
    "lstm = nn.LSTM(input_size, hidden_size, num_layers=1) #lstm layer\n",
    "out = nn.Linear(hidden_size, output_size)\n",
    "softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "# loss_function = nn.NLLLoss()\n",
    "# optimizer = optim.SGD( lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = train_data[2]\n",
    "input_idx = [model1.wv.vocab[x].index for x in input_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word: <sos>\n",
      "predicted_class: <eos>\n",
      "input word: mr.\n",
      "predicted_class: million\n",
      "input word: <unk>\n",
      "predicted_class: 's\n",
      "input word: is\n",
      "predicted_class: its\n",
      "input word: chairman\n",
      "predicted_class: inc.\n",
      "input word: of\n",
      "predicted_class: after\n",
      "input word: <unk>\n",
      "predicted_class: when\n",
      "input word: n.v.\n",
      "predicted_class: if\n",
      "input word: the\n",
      "predicted_class: corp.\n",
      "input word: dutch\n",
      "predicted_class: have\n",
      "input word: publishing\n",
      "predicted_class: sales\n",
      "input word: group\n",
      "predicted_class: after\n",
      "input word: <eos>\n",
      "predicted_class: after\n"
     ]
    }
   ],
   "source": [
    "# intialise first hidden state\n",
    "(hidden, cell) = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
    "\n",
    "for idx in input_idx:\n",
    "    print(\"input word:\", model1.wv.index2word[idx])\n",
    "    output = embedded(torch.tensor([idx], dtype=torch.long)).view(1, 1, 100)\n",
    "    output, (hidden, cell) = lstm(output, (hidden, cell))\n",
    "    output = softmax(out(output[0]))\n",
    "    predicted_idx = np.argmax(output.detach().numpy())\n",
    "    print(\"predicted_class: {0}\".format(model1.wv.index2word[predicted_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn_language_model(nn.Module):\n",
    "    def __init__(self, vocabulary_size, input_size, hidden_size, num_layers, weights):\n",
    "        super(rnn_language_model, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(weights)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocabulary_size)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x = self.embed(torch.tensor(x, dtype=torch.long)).view(1, len(x), 100)\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        out = self.linear(out)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "num_layers = 1\n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "\n",
    "model = rnn_language_model(vocabulary_size, input_size, hidden_size, num_layers, weights)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Fixed batch size - this should probably be changed later since the size will vary from sentence to sentence, not sure how...\n",
    "def get_batch(data, batch_size):\n",
    "    batches = []\n",
    "    count = 0\n",
    "    current_batch = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            if count == batch_size + 1:\n",
    "                batches.append(current_batch)\n",
    "                current_batch = []\n",
    "                count = 0\n",
    "            else:\n",
    "                current_batch.append(data[i][j])\n",
    "                count += 1\n",
    "    return batches  \n",
    "\n",
    "batches = get_batch(train_data[1:100], batch_size)\n",
    "print(len(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "Epoch [1/100], Loss: 9.1851, Perplexity: 9750.43\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "Epoch [2/100], Loss: 9.1781, Perplexity: 9682.86\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "Epoch [3/100], Loss: 9.1711, Perplexity: 9615.25\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(epochs):\n",
    "    (hidden, cell) = (torch.zeros(num_layers, batch_size, hidden_size), torch.zeros(num_layers, batch_size, hidden_size))\n",
    "    for i in range(len(batches)):\n",
    "        #for j in range(len(train_data[i]) - 1):\n",
    "        inputs = [model1.wv.vocab[x].index for x in batches[i][:-1]]\n",
    "        targets = torch.tensor([model1.wv.vocab[x].index for x in batches[i][1:]], dtype=torch.long)\n",
    "\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "    \n",
    "        outputs, (hidden, cell) = model(inputs, (hidden, cell))\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "               .format(epoch+1, epochs, loss.item(), np.exp(loss.item())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
