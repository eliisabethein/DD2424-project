{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25fe22efa50>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Antonio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = TweetTokenizer(preserve_case=False)\n",
    "tok = MWETokenizer([('<', 'unk', '>')], separator = '')\n",
    "\n",
    "train_data_padded = []\n",
    "train_data_not_padded = []\n",
    "vocabulary = []\n",
    "lengths_of_sentences = []\n",
    "maximum_sentence_length = 50\n",
    "\n",
    "with open(\"data/ptb.train.txt\") as f:\n",
    "    total_sentences = 0\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        s = word_tokenize(line)\n",
    "        s = tok.tokenize(s)[:maximum_sentence_length - 2]\n",
    "        s = ['<sos>'] + s + ['<eos>']\n",
    "        train_data_not_padded.append(s.copy())\n",
    "        lengths_of_sentences.append(len(s))\n",
    "        s.extend(['<pad>']*(maximum_sentence_length - len(s))) \n",
    "        total_sentences += 1\n",
    "        train_data_padded.append(s)\n",
    "        vocabulary += s\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>']\n",
      "total sentences: 42068\n"
     ]
    }
   ],
   "source": [
    "longest_sentence_size = [len(x) for x in train_data_padded]\n",
    "# print((longest_sentence_size))\n",
    "# print(lengths_of_sentences)\n",
    "print(train_data_not_padded[0])\n",
    "\n",
    "vocabulary = sorted(list(set(vocabulary)))\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "print(\"total sentences:\", total_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66651102, 96960000)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "embedding_size = 100\n",
    "word_embeddings = Word2Vec(train_data_not_padded, min_count=1, size=embedding_size, window=5)\n",
    "word_embeddings.train(train_data_not_padded, len(train_data_not_padded), epochs=100, \n",
    "                      total_examples=word_embeddings.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loans', 0.517526388168335),\n",
       " ('loan', 0.4132787585258484),\n",
       " ('financial', 0.4076409935951233),\n",
       " ('debt', 0.4062206745147705),\n",
       " ('less-developed', 0.4037669599056244),\n",
       " ('savings', 0.3898395299911499),\n",
       " ('write-downs', 0.3844359517097473),\n",
       " ('payment', 0.38194894790649414),\n",
       " ('capital', 0.38089674711227417),\n",
       " ('financing', 0.3681441843509674)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_embeddings.wv.most_similar(\"credit\")\n",
    "# word_embeddings.wv['credit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(vocabulary) - 1 # - 1 for <'pad'>\n",
    "input_size = 100\n",
    "output_size = 100\n",
    "hidden_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', 'aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '<eos>']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9968"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the word embeddings into a pythorch tensor\n",
    "weights = torch.FloatTensor(word_embeddings.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn_language_model(nn.Module):\n",
    "    def __init__(self, vocabulary_size, input_size, hidden_size, num_layers, weights):\n",
    "        super(rnn_language_model, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(weights)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocabulary_size)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x = self.embed(torch.tensor(x, dtype=torch.long)).view(1, len(x), 100)\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        out = self.linear(out)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9538\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "num_layers = 1\n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "\n",
    "model = rnn_language_model(vocabulary_size, input_size, hidden_size, num_layers, weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Fixed batch size - this should probably be changed later since the size will vary from sentence to sentence, not sure how...\n",
    "def get_batch(data, batch_size):\n",
    "    batches = []\n",
    "    count = 0\n",
    "    current_batch = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            if count == batch_size + 1:\n",
    "                batches.append(current_batch)\n",
    "                current_batch = []\n",
    "                count = 0\n",
    "            else:\n",
    "                current_batch.append(data[i][j])\n",
    "                count += 1\n",
    "    return batches  \n",
    "\n",
    "batches = get_batch(train_data, batch_size)\n",
    "print(len(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   2, 9967, 9968, 9969, 9970, 9971, 9972, 9973, 9974, 9975, 9976, 9977,\n",
      "        9978, 9979, 9980, 9981, 9982, 9983, 9984, 9985, 9986, 9987, 9988, 9989,\n",
      "        9990,    3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-110-43e8caf35915>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_data_not_padded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_not_padded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m X = pack_padded_sequence(temp,\n\u001b[1;32m----> 8\u001b[1;33m                          lengths_of_sentences, batch_first=True)\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\torch\\nn\\utils\\rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[1;34m(input, lengths, batch_first)\u001b[0m\n\u001b[0;32m    146\u001b[0m                       category=torch.jit.TracerWarning, stacklevel=2)\n\u001b[0;32m    147\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pack_padded_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# constructing a batch for the forward pass\n",
    "# print([word_embeddings.wv.vocab[x].index for x in train_data_padded[:2]])\n",
    "for i in range(len(train_data_padded))\n",
    "temp = torch.tensor([word_embeddings.wv.vocab[x].index for x in train_data_not_padded[0]])\n",
    "#                      for i in range(len(train_data_not_padded))])\n",
    "print(temp)\n",
    "temp = torch.tensor([word_embeddings.wv.vocab[x].index for x in train_data_not_padded[i] for i in range(len(train_data_not_padded))])\n",
    "X = pack_padded_sequence(temp,\n",
    "                         lengths_of_sentences, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-84-d1316243506b>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-84-d1316243506b>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    inputs = [word_embeddings.wv.vocab[x].index for x in batches[i][:-1] if x is not '<pad>' else torch.FloatTensor(np.zeros(embedding_size))]\u001b[0m\n\u001b[1;37m                                                                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(epochs):\n",
    "    (hidden, cell) = (torch.zeros(num_layers, batch_size, hidden_size), torch.zeros(num_layers, batch_size, hidden_size))\n",
    "    for i in range(len(batches)):\n",
    "        inputs = [word_embeddings.wv.vocab[x].index for x in batches[i][:-1] if x is not '<pad>' else np.zeros(embedding_size)]\n",
    "        targets = torch.tensor([word_embeddings.wv.vocab[x].index for x in batches[i][1:] if x is not '<pad>' else np.zeros(embedding_size)], dtype=torch.long)\n",
    "\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "    \n",
    "        outputs, (hidden, cell) = model(inputs, (hidden, cell))\n",
    "        loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "               .format(epoch, epochs, loss.item(), np.exp(loss.item())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word: <unk>\n",
      "predicted_class: <sos>\n",
      "input word: with\n",
      "predicted_class: at\n",
      "input word: the\n",
      "predicted_class: were\n",
      "input word: $\n",
      "predicted_class: do\n",
      "input word: chairman\n",
      "predicted_class: shares\n",
      "input word: N\n",
      "predicted_class: its\n",
      "input word: the\n",
      "predicted_class: were\n",
      "input word: rewards\n",
      "predicted_class: its\n",
      "input word: <pad>\n",
      "predicted_class: shares\n",
      "input word: indicators\n",
      "predicted_class: co.\n",
      "input word: chips\n",
      "predicted_class: its\n",
      "input word: co.\n",
      "predicted_class: shares\n",
      "input word: <sos>\n",
      "predicted_class: &\n"
     ]
    }
   ],
   "source": [
    "# intialise first hidden state\n",
    "(hidden, cell) = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
    "\n",
    "for idx in input_idx:\n",
    "    print(\"input word:\", word_embeddings.wv.index2word[idx])\n",
    "    output = embedded(torch.tensor([idx], dtype=torch.long)).view(1, 1, 100)\n",
    "    output, (hidden, cell) = lstm(output, (hidden, cell))\n",
    "    output = softmax(out(output[0]))\n",
    "    predicted_idx = np.argmax(output.detach().numpy())\n",
    "    print(\"predicted_class: {0}\".format(word_embeddings.wv.index2word[predicted_idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
