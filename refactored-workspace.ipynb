{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "HpqqiuiZbZLn",
    "outputId": "4feaa888-53de-480d-d7d3-e2ffd7ad7a2b"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributions.normal as normal\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "import csv\n",
    "from gensim.models import Word2Vec\n",
    "import os.path\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JqN4walGbZLr"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cSKWwvR2bZLu"
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6k0Znrz6bZLu"
   },
   "outputs": [],
   "source": [
    "def load_data(filename, max_sentence_len, with_labels=False):\n",
    "    # the tokenizer splits <unk> so we use MWETokenizer to re-merge it\n",
    "    data_original = []\n",
    "    data_padded = []\n",
    "    with open(filename, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            sentence, padded_sentence = tokenize_sentence(line, max_sentence_len, with_labels)\n",
    "            data_original.append(sentence)\n",
    "            data_padded.append(padded_sentence)\n",
    "    \n",
    "    return data_original, data_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tRPRgIpbZLx"
   },
   "outputs": [],
   "source": [
    "def tokenize_sentence(string, max_sentence_len, with_labels=False, occurrences=None):\n",
    "    merger = MWETokenizer([('<', 'unk', '>')], separator = '') \n",
    "    sentence = word_tokenize(string.strip())       # tokenize sentence\n",
    "    sentence = merger.tokenize(sentence)         # merge <unk>\n",
    "    if with_labels:\n",
    "        sentence = sentence[1:]\n",
    "    sentence = [token.lower() for token in sentence]            \n",
    "    sentence = sentence[:max_sentence_len - 2]   # cut sentence at max_sentence_length    \n",
    "    sentence = ['<sos>'] + sentence + ['<eos>']  # add start and end-of-sentence tags\n",
    "    if occurrences is not None:\n",
    "        for word in sentence:\n",
    "            if word in occurrences:\n",
    "                occurrences[word] += 1\n",
    "            else:\n",
    "                occurrences[word] = 1\n",
    "\n",
    "    # pad the rest of the sentence\n",
    "    padded_sentence = sentence.copy()\n",
    "    padded_sentence.extend(['<pad>']*(max_sentence_len - len(sentence))) \n",
    "    \n",
    "    if occurrences is not None:\n",
    "        return sentence, padded_sentence, occurrences\n",
    "    else:\n",
    "        return sentence, padded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "50fGsen8bZL0"
   },
   "outputs": [],
   "source": [
    "def get_batches_text(data, data_padded, batch_size, pad_index, word2vec_model, unk_word='<unk>'):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    lengths = []\n",
    "    for i in range(len(data) // batch_size):\n",
    "        # take batch_size sentences from the data each time\n",
    "        batch_sentences = data[i*batch_size:(i+1)*batch_size]\n",
    "        batch_sentence_lens = [len(x) for x in batch_sentences]\n",
    "        \n",
    "        # sentences in a batch have to be sorted in decreasing order of length (for pack_padded_sentence)\n",
    "        sorted_pairs = sorted(zip(batch_sentence_lens,batch_sentences), reverse=True)\n",
    "        batch_sentences = [sentence for length, sentence in sorted_pairs]\n",
    "        batch_sentence_lens = [length-1 for length, sentence in sorted_pairs]\n",
    "        \n",
    "        # each input and target is a (batch_size x max_sentence_len-1 x 1) matrix\n",
    "        # initially filled with the index for padditng tag <pad>\n",
    "        input_batch = np.ones((batch_size, len(data_padded[0])-1, 1)) * pad_index\n",
    "        target_batch = np.ones((batch_size, len(data_padded[0])-1, 1)) * pad_index\n",
    "        \n",
    "        # for each sentence in the batch, fill the corresponding row in current_batch\n",
    "        # with the indexed of the words in the sentence (except for <pad>)\n",
    "        for j, sentence in enumerate(batch_sentences):\n",
    "            word_indexes = np.array([word2vec_model.wv.vocab[word].index if word in word2vec_model.wv.vocab else word2vec_model.wv.vocab[unk_word].index for word in sentence])\n",
    "            input_batch[j,0:len(sentence)-1,0] = word_indexes[:-1]\n",
    "            target_batch[j,0:len(sentence)-1,0] = word_indexes[1:]\n",
    "        \n",
    "        # make the matrices into torch tensors and append\n",
    "        inputs.append(input_batch)\n",
    "        targets.append(target_batch)\n",
    "        lengths.append(batch_sentence_lens)\n",
    "    return inputs, targets, lengths\n",
    "\n",
    "def get_batches_synthetic(data, batch_size):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for i in range(len(data) // batch_size):\n",
    "        batch_sentences = data[i * batch_size:(i+1) * batch_size]\n",
    "\n",
    "        input_batch = np.ones((batch_size, data.shape[1] - 1, 1)) \n",
    "        target_batch = np.ones((batch_size, data.shape[1] - 1, 1)) \n",
    "        for j, sentence in enumerate(batch_sentences):\n",
    "                input_batch[j,0:len(sentence)-1,0] = sentence[:-1]\n",
    "                target_batch[j,0:len(sentence)-1,0] = sentence[1:]\n",
    "        inputs.append(input_batch)\n",
    "        targets.append(target_batch)\n",
    "    \n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QTfmo5X7bZMB"
   },
   "source": [
    "### Encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJRr0EEBbZMB"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, embedding_weights, synthetic=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        # parameters\n",
    "        self.embedding_size = embedding_weights.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = True\n",
    "        \n",
    "        #layers\n",
    "        self.embed = nn.Embedding.from_pretrained(embedding_weights)\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, batch_first=self.batch_first)\n",
    "        \n",
    "    def forward(self, x, hidden, x_lens=None, train=True):\n",
    "        batch_size, max_len, _ = x.shape\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.long)  # make the input into a torch tensor\n",
    "        x = self.embed(x).view(batch_size, max_len, self.embedding_size)\n",
    "\n",
    "        if x_lens is not None and train:\n",
    "            x_lens = torch.tensor(x_lens, dtype=torch.long)\n",
    "            x = pack_padded_sequence(x, x_lens, batch_first=self.batch_first)\n",
    "            \n",
    "        output, hidden = self.lstm(x.float(), hidden) \n",
    "\n",
    "        if x_lens is not None and train:\n",
    "            output, output_lens = pad_packed_sequence(output, batch_first=self.batch_first, \n",
    "                                                      total_length=max_sentence_length-1)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        h = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        c = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "plSlS-fubZME"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,hidden_size, num_layers, embedding_weights, synthetic=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        # parameters\n",
    "        self.vocabulary_size = embedding_weights.shape[0]\n",
    "        self.embedding_size = embedding_weights.shape[1]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = True\n",
    "        \n",
    "        # layers\n",
    "        self.embed = nn.Embedding.from_pretrained(embedding_weights)\n",
    "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.num_layers, batch_first=self.batch_first)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.vocabulary_size)\n",
    "\n",
    "    def forward(self, x, hidden, x_lens=None, train=True):\n",
    "        batch_size, max_len, _ = x.shape\n",
    "        \n",
    "        x = torch.tensor(x, dtype=torch.long)  # make the input into a torch tensor\n",
    "        x = self.embed(x).view(batch_size, max_len, self.embedding_size)\n",
    "        \n",
    "        if x_lens is not None and train:\n",
    "            x_lens = torch.tensor(x_lens, dtype=torch.long)\n",
    "            x = pack_padded_sequence(x, x_lens, batch_first=self.batch_first)\n",
    "\n",
    "        output, hidden = self.lstm(x.float(), hidden) \n",
    "        \n",
    "        if x_lens is not None and train:\n",
    "            output, output_lens = pad_packed_sequence(output, batch_first=self.batch_first, \n",
    "                                                      total_length=max_sentence_length-1)\n",
    "        \n",
    "        output = output.reshape(output.size(0)*output.size(1), output.size(2))\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RGTiY8OzbZMG"
   },
   "source": [
    "### VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XBA-nbGTbZMH"
   },
   "outputs": [],
   "source": [
    "# class Stochastic(nn.Module):\n",
    "#     def __init__(self, hidden_dim, num_layers, latent_dim, synthetic=False):\n",
    "#         super(Stochastic, self).__init__()\n",
    "#         self.latent_dim = latent_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.batch_first = True\n",
    "        \n",
    "#         self.hidden_to_mean = nn.Linear(2 * self.hidden_dim * num_layers, self.latent_dim, self.batch_first)\n",
    "#         self.hidden_to_logvar = nn.Linear(2 * self.hidden_dim * num_layers, self.latent_dim, self.batch_first)\n",
    "#         self.latent_to_hidden = nn.Linear(latent_dim, 2 * self.hidden_dim * num_layers, self.batch_first)\n",
    "        \n",
    "# #         if synthetic:          \n",
    "# #             for param in self.parameters():\n",
    "# #                 nn.init.uniform_(param, -0.01, 0.01)\n",
    "\n",
    "#     def reparametrize(self, mean, log_variance):\n",
    "#         eps = torch.randn_like(mean)\n",
    "#         return mean + eps * torch.exp(0.5 * log_variance)\n",
    "        \n",
    "#     def forward(self, hidden_concatenated):\n",
    "#         mean = self.hidden_to_mean(hidden_concatenated)\n",
    "#         log_variance = self.hidden_to_logvar(hidden_concatenated)\n",
    "#         z = self.reparametrize(mean, log_variance)\n",
    "#         hidden_concatenated = self.latent_to_hidden(z)\n",
    "#         return hidden_concatenated, mean, log_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HVeJWdxDbZMJ"
   },
   "outputs": [],
   "source": [
    "class StochasticEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, latent_dim, synthetic=False):\n",
    "        super(StochasticEncoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_first = True\n",
    "        \n",
    "        self.hidden_to_mean = nn.Linear(2 * self.hidden_dim * num_layers, self.latent_dim, self.batch_first)\n",
    "        self.hidden_to_logvar = nn.Linear(2 * self.hidden_dim * num_layers, self.latent_dim, self.batch_first)\n",
    "\n",
    "    def reparametrize(self, mean, log_variance):\n",
    "        eps = torch.randn_like(mean)\n",
    "        return mean + eps * torch.exp(0.5 * log_variance)\n",
    "        \n",
    "    def forward(self, hidden_concatenated):\n",
    "        mean = self.hidden_to_mean(hidden_concatenated)\n",
    "        log_variance = self.hidden_to_logvar(hidden_concatenated)\n",
    "        z = self.reparametrize(mean, log_variance)\n",
    "        return z, mean, log_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eWVMhlE_bZMK"
   },
   "outputs": [],
   "source": [
    "class StochasticDecoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, latent_dim, synthetic=False):\n",
    "        super(StochasticDecoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_first = True\n",
    "        \n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, 2 * self.hidden_dim * num_layers, self.batch_first)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        hidden_concatenated = self.latent_to_hidden(z)\n",
    "        return hidden_concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a92xK-5ObZMM"
   },
   "outputs": [],
   "source": [
    "# # new VAE\n",
    "# class VAE(nn.Module):\n",
    "#     def __init__(self, hidden_dim, num_layers, embedding_weights, latent_dim, synthetic=False):\n",
    "#         super(VAE, self).__init__()\n",
    "#         self.latent_dim = latent_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.batch_first = True\n",
    "        \n",
    "#         self.encoder = Encoder(self.hidden_dim, num_layers, embedding_weights, synthetic)\n",
    "#         self.stochastic = Stochastic(self.hidden_dim, num_layers, self.latent_dim, synthetic)\n",
    "#         self.decoder = Decoder(self.hidden_dim, num_layers, embedding_weights, synthetic)\n",
    "        \n",
    "#         # THIS PART IS IMPORTANT -- I think it re-initialises all of the weights in the network with\n",
    "#         ## this distribution, even the embedding weights, which we initialised to (-0.1,0.1) before;\n",
    "#         ## with this distribution it (kind of) works both without annealing and with annealing \n",
    "#         ## (why??? who knows)\n",
    "#         if synthetic:          \n",
    "#             for param in self.parameters():\n",
    "#                 nn.init.uniform_(param, -0.01, 0.01)\n",
    "        \n",
    "#     def encode(self, x, x_lens=None):\n",
    "#         batch_size, max_len, _ = x.shape\n",
    "#         hidden = self.encoder.init_hidden(batch_size)\n",
    "#         _, hidden = self.encoder.forward(x, hidden, x_lens)\n",
    "#         return hidden\n",
    "    \n",
    "#     def latent_to_hidden(self, x):\n",
    "#         return self.stochastic.latent_to_hidden(x)\n",
    "        \n",
    "#     def decode(self, hidden, x, x_lens=None, train=True):\n",
    "#         outputs, _ = self.decoder.forward(x, hidden, x_lens, train)\n",
    "#         return outputs\n",
    "    \n",
    "#     def forward(self, x, x_lens=None):\n",
    "#         hidden = self.encode(x, x_lens)\n",
    "#         hidden_concatenated = torch.cat((hidden[0], hidden[1]), 2)\n",
    "#         hidden_concatenated, mean, log_variance = self.stochastic(hidden_concatenated)\n",
    "#         hidden = torch.split(hidden_concatenated, self.hidden_dim, dim=2)\n",
    "#         outputs = self.decode(hidden, x, x_lens)\n",
    "#         return mean, log_variance, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pmtmMPFjbZMO"
   },
   "outputs": [],
   "source": [
    "# new VAE\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, embedding_weights, latent_dim, synthetic=False):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_first = True\n",
    "        \n",
    "        self.encoder = Encoder(self.hidden_dim, num_layers, embedding_weights, synthetic)\n",
    "        self.stochastic_encoder = StochasticEncoder(self.hidden_dim, num_layers, self.latent_dim, synthetic)\n",
    "        self.stochastic_decoder = StochasticDecoder(self.hidden_dim, num_layers, self.latent_dim, synthetic)\n",
    "        self.decoder = Decoder(self.hidden_dim, num_layers, embedding_weights, synthetic)\n",
    "        \n",
    "        # THIS PART IS IMPORTANT -- I think it re-initialises all of the weights in the network with\n",
    "        ## this distribution, even the embedding weights, which we initialised to (-0.1,0.1) before;\n",
    "        ## with this distribution it (kind of) works both without annealing and with annealing \n",
    "        ## (why??? who knows)\n",
    "        if synthetic:          \n",
    "            for param in self.parameters():\n",
    "                nn.init.uniform_(param, -0.01, 0.01)\n",
    "            nn.init.uniform_(self.encoder.embed.weight, -0.1, 0.1)\n",
    "            nn.init.uniform_(self.decoder.embed.weight, -0.1, 0.1)\n",
    "        \n",
    "    def encode(self, x, x_lens=None):\n",
    "        batch_size, max_len, _ = x.shape\n",
    "        hidden = self.encoder.init_hidden(batch_size)\n",
    "        _, hidden = self.encoder.forward(x, hidden, x_lens)\n",
    "        return hidden\n",
    "    \n",
    "    def latent_to_hidden(self, z):\n",
    "        return self.stochastic_decoder.latent_to_hidden(z)\n",
    "        \n",
    "    # with teacher forcing\n",
    "    def decode(self, hidden, x, x_lens=None, train=True): \n",
    "        outputs, _ = self.decoder.forward(x, hidden, x_lens, train)\n",
    "        return outputs\n",
    "    \n",
    "    def forward(self, x, x_lens=None, dropout_rate=0, unk_index=0):\n",
    "        hidden = self.encode(x, x_lens)\n",
    "        hidden_concatenated = torch.cat((hidden[0], hidden[1]), 2)\n",
    "        z, mean, log_variance = self.stochastic_encoder.forward(hidden_concatenated)\n",
    "        hidden_concatenated = self.stochastic_decoder.forward(z)\n",
    "        hidden = torch.split(hidden_concatenated, self.hidden_dim, dim=2)\n",
    "        \n",
    "        # word dropout\n",
    "        if dropout_rate != 0.0:\n",
    "            drop_probs = np.random.random_sample(x.shape)\n",
    "            drop_probs[:, 0] = 1                  # set the <sos> token to 1 (always keep)\n",
    "            for i, x_len in enumerate(x_lens):\n",
    "                drop_probs[i, x_len:, 0] = 1      # set the <pad> tokens to 1 (always keep)\n",
    "            dropped_x = x.copy()\n",
    "            dropped_x[drop_probs < dropout_rate] = unk_index\n",
    "            outputs = self.decode(hidden, dropped_x, x_lens)\n",
    "        else:\n",
    "            outputs = self.decode(hidden, x, x_lens)\n",
    "            \n",
    "        return mean, log_variance, outputs\n",
    "    \n",
    "    def calc_mi(self, x):\n",
    "        # I(x, z) = E_xE_{q(z|x)}log(q(z|x)) - E_xE_{q(z|x)}log(q(z))\n",
    "        mean, log_variance, _ = self.forward(x)\n",
    "        _, batch_size, _ = mean.size()\n",
    "\n",
    "        # E_{q(z|x)}log(q(z|x)) = -0.5*nz*log(2*\\pi) - 0.5*(1+logvar).sum(-1)\n",
    "        neg_entropy = (-0.5 * self.latent_dim * np.log(2 * np.pi)- 0.5 * (1 + log_variance).sum(-1)).mean()\n",
    "\n",
    "        z = self.stochastic_encoder.reparametrize(mean, log_variance)\n",
    "        mean, log_variance = mean.unsqueeze(0), log_variance.unsqueeze(0)\n",
    "\n",
    "        log_density = -0.5 * (((z - mean) ** 2) / log_variance.exp()).sum(dim=-1) - \\\n",
    "            0.5 * (self.latent_dim * np.log(2 * np.pi) + log_variance.sum(-1))\n",
    "\n",
    "        log_qz = log_sum_exp(log_density, dim=1) - np.log(batch_size)\n",
    "\n",
    "        return (neg_entropy - log_qz.mean(-1)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SApA3jcebZMQ"
   },
   "outputs": [],
   "source": [
    "# # old VAE\n",
    "# class VAE(nn.Module):\n",
    "#     def __init__(self, hidden_dim, num_layers, embedding_weights, latent_dim, synthetic=False):\n",
    "#         super(VAE, self).__init__()\n",
    "#         self.encoder = Encoder(hidden_dim, num_layers, embedding_weights, synthetic)\n",
    "#         self.decoder = Decoder(hidden_dim, num_layers, embedding_weights, synthetic)\n",
    "#         self.latent_dim = latent_dim\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.batch_first = True\n",
    "        \n",
    "#         self.hidden_to_mean = nn.Linear(2 * self.hidden_dim * num_layers, self.latent_dim, self.batch_first)\n",
    "#         self.hidden_to_logvar = nn.Linear(2 * self.hidden_dim * num_layers, self.latent_dim, self.batch_first)\n",
    "#         self.latent_to_hidden = nn.Linear(latent_dim, 2 * self.hidden_dim * num_layers, self.batch_first)\n",
    "#         if synthetic:          \n",
    "#             for param in self.parameters():\n",
    "#                 nn.init.uniform_(param, -0.01, 0.01)\n",
    "        \n",
    "#     def reparametrize(self, mean, log_variance):\n",
    "#         eps = torch.randn_like(mean)\n",
    "#         return mean + eps * torch.exp(0.5 * log_variance)\n",
    "        \n",
    "#     def encode(self, x, x_lens=None):\n",
    "#         batch_size, max_len, _ = x.shape\n",
    "#         hidden = self.encoder.init_hidden(batch_size)\n",
    "#         _, hidden = self.encoder.forward(x, hidden, x_lens)\n",
    "#         return hidden\n",
    "        \n",
    "#     def decode(self, hidden, x, x_lens=None, train=True):\n",
    "#         outputs, _ = self.decoder.forward(x, hidden, x_lens, train)\n",
    "#         return outputs\n",
    "\n",
    "#     def forward(self, x, x_lens=None):\n",
    "#         hidden = self.encode(x, x_lens)\n",
    "#         hidden_concatenated = torch.cat((hidden[0], hidden[1]), 2)\n",
    "        \n",
    "#         mean = self.hidden_to_mean(hidden_concatenated)\n",
    "        \n",
    "#         log_variance = self.hidden_to_logvar(hidden_concatenated)\n",
    "#         z = self.reparametrize(mean, log_variance)\n",
    "#         hidden = self.latent_to_hidden(z)\n",
    "        \n",
    "#         hidden = torch.split(hidden, self.hidden_dim, dim=2)\n",
    "#         outputs = self.decode(hidden, x, x_lens)\n",
    "#         return mean, log_variance, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ozU8w8EbZMT"
   },
   "outputs": [],
   "source": [
    "# def loss_function(outputs, labels, mean, log_variance, annealing_args=None):\n",
    "#     BCE = nn.CrossEntropyLoss(reduction='sum')(outputs, labels)\n",
    "#     KLD = -0.5 * torch.sum(1 + log_variance - mean.pow(2) - log_variance.exp()) \n",
    "#     if annealing_args is not None:\n",
    "#         kl_weight = kl_annealing_weight(annealing_args['type'], annealing_args['step'], annealing_args['k'], annealing_args['first_step'])\n",
    "#     else:\n",
    "#         kl_weight = 1.0\n",
    "#     BCE = BCE / mean.shape[1]  # divide by batch size\n",
    "#     KLD /= mean.shape[1]\n",
    "#     weighted_KLD = kl_weight * KLD\n",
    "#     loss = BCE + weighted_KLD\n",
    "#     return loss, BCE, KLD, weighted_KLD, kl_weight\n",
    "    \n",
    "# def kl_annealing_weight(annealing_type, step, k, first_step):\n",
    "#     if annealing_type == 'logistic':\n",
    "#         return float(1/(1+np.exp(-k*(step-first_step))))\n",
    "#     elif annealing_type == 'linear':\n",
    "#         return min(1, step/first_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "11ksRu_ibZMV"
   },
   "outputs": [],
   "source": [
    "def loss_function(outputs, labels, mean, log_variance, seq_length, annealing_args=None, mask=None):\n",
    "    if mask is not None:\n",
    "        BCE = torch.zeros(mean.shape[1] * (seq_length - 1))\n",
    "        BCE[mask] = nn.CrossEntropyLoss(reduction='none')(outputs, labels)\n",
    "    else:\n",
    "        BCE = nn.CrossEntropyLoss(reduction='none')(outputs, labels)\n",
    "    BCE = BCE.view(mean.shape[1], -1).sum(-1)\n",
    "    KLD = -0.5 * (1 + log_variance - mean.pow(2) - log_variance.exp()).permute(1, 0, 2).sum(-1).squeeze(-1)\n",
    "    if annealing_args is not None:\n",
    "        kl_weight = kl_annealing_weight(annealing_args['type'], annealing_args['step'], annealing_args['k'], annealing_args['first_step'])\n",
    "    else:\n",
    "        kl_weight = 1.0\n",
    "    weighted_KLD = kl_weight * KLD\n",
    "    loss = BCE + weighted_KLD\n",
    "    return loss, BCE, KLD, weighted_KLD, kl_weight\n",
    "    \n",
    "def kl_annealing_weight(annealing_type, step, k, first_step):\n",
    "    if annealing_type == 'logistic':\n",
    "        return float(1/(1+np.exp(-k*(step-first_step))))\n",
    "    elif annealing_type == 'linear':\n",
    "        return min(1, step/first_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_interpolation_sequence(z1, z2, number_of_sentences_to_decode):\n",
    "    interpolations = np.zeros((z1.shape[0], number_of_sentences_to_decode + 2))\n",
    "    for dimension_of_z, (i, j) in enumerate(zip(z1, z2)):\n",
    "        interpolations[dimension_of_z] = np.linspace(i, j, number_of_sentences_to_decode + 2)    \n",
    "    return interpolations.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_PQofHPbZMZ"
   },
   "source": [
    "### True posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9mLWcpwQbZMZ"
   },
   "outputs": [],
   "source": [
    "def log_sum_exp(value, dim=None, keepdim=False):\n",
    "    \"\"\"Numerically stable implementation of the operation\n",
    "    value.exp().sum(dim, keepdim).log() - copied from repo, we should change it\n",
    "    \"\"\"\n",
    "    if dim is not None:\n",
    "        m, _ = torch.max(value, dim=dim, keepdim=True)\n",
    "        value0 = value - m\n",
    "        if keepdim is False:\n",
    "            m = m.squeeze(dim)\n",
    "        return m + torch.log(torch.sum(torch.exp(value0), dim=dim, keepdim=keepdim))\n",
    "    else:\n",
    "        m = torch.max(value)\n",
    "        sum_exp = torch.sum(torch.exp(value - m))\n",
    "        return m + torch.log(sum_exp)\n",
    "\n",
    "def compute_true_posterior(latent_grid, vae, inputs, targets):\n",
    "    log_true_posterior = compute_true_log_posterior(latent_grid, vae, inputs, targets)\n",
    "    true_posterior = log_true_posterior.exp()\n",
    "    return true_posterior\n",
    "\n",
    "def compute_true_log_posterior(latent_grid, vae, inputs, targets):\n",
    "    latent_grid = latent_grid.unsqueeze(0).expand(inputs.shape[0], *latent_grid.size()).contiguous().permute(1, 0, 2)\n",
    "    \n",
    "    # Compute the true joint\n",
    "    log_true_joint = compute_true_joint(latent_grid, vae, inputs, targets)\n",
    "    \n",
    "    # Normalize by marginalizing z\n",
    "    log_true_posterior = log_true_joint - log_sum_exp(log_true_joint, dim=0, keepdim=True)\n",
    "    return log_true_posterior\n",
    "\n",
    "def compute_true_joint(latent_grid, vae, inputs, targets):\n",
    "    n_sample, batch_size, latent_dim = latent_grid.size()\n",
    "    seq_len = inputs.shape[1]\n",
    "    # Compute prior p(z)\n",
    "    normal = torch.distributions.normal.Normal(torch.zeros(latent_dim), torch.ones(latent_dim))\n",
    "    log_true_prior = normal.log_prob(latent_grid).sum(dim=-1)\n",
    "    \n",
    "    # Compute conditional p(x | z)\n",
    "    log_true_conditional = torch.zeros(latent_grid.size(0), latent_grid.size(1))\n",
    "    tensor_target_batch = torch.tensor(targets.reshape(-1), dtype=torch.long)\n",
    "    for i in range(latent_grid.size(0)):\n",
    "        hidden_concatenated = vae.latent_to_hidden(latent_grid[i]).unsqueeze(0)\n",
    "        hidden = torch.split(hidden_concatenated, vae.hidden_dim, dim=-1)\n",
    "        outputs = vae.decode(hidden, inputs, train=False)\n",
    "        log_true_conditional[i] = -nn.CrossEntropyLoss(reduction='none')(outputs, tensor_target_batch).view(batch_size, -1).sum(-1)\n",
    "        \n",
    "    # Compute joint p(x, z)\n",
    "    log_true_joint = log_true_prior + log_true_conditional\n",
    "    return log_true_joint\n",
    "\n",
    "def compute_true_posterior_mean(true_posterior, latent_grid):\n",
    "    return torch.mul(true_posterior.unsqueeze(2), latent_grid.unsqueeze(0)).sum(1)\n",
    "\n",
    "def generate_grid(lower, upper, step, dim=2):\n",
    "    line = torch.arange(lower, upper, step)\n",
    "    total_points = line.size(0)\n",
    "    if dim == 2:\n",
    "        z1 = line.unsqueeze(1).repeat(1, total_points).view(-1)\n",
    "        z2 = line.repeat(total_points)\n",
    "        return torch.cat((z1.unsqueeze(-1), z2.unsqueeze(-1)), dim=-1)\n",
    "    elif dim == 1:\n",
    "        return line.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-MX0DvhbZMc"
   },
   "outputs": [],
   "source": [
    "def plot_mean_space(step, latent_size, vae, tracked_inputs, tracked_targets, lim=3, iteration=None):\n",
    "    latent_grid = generate_grid(-5, 5, step, latent_size)\n",
    "    true_posterior = compute_true_posterior(latent_grid, vae, tracked_inputs, tracked_targets)\n",
    "    true_mean = compute_true_posterior_mean(true_posterior.t(), latent_grid)\n",
    "    vae.eval()\n",
    "    approximate_mean, _, _ = vae.forward(tracked_inputs)\n",
    "    vae.train()\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.scatter(true_mean.detach().numpy(), approximate_mean.detach().numpy(), marker='x')\n",
    "    plt.xlim(-lim, lim)\n",
    "    plt.ylim(-lim, lim)\n",
    "    plt.xlabel(\"true postrior mean\")\n",
    "    plt.ylabel(\"approximate posterior mean\")\n",
    "    if iteration is not None:\n",
    "        plt.title(\"iteration {0}\".format(iteration))\n",
    "    plt.show()\n",
    "\n",
    "def plot_kl(kl_terms, kl_weights):\n",
    "    plot_step = 10\n",
    "    x_axis = np.arange(len(kl_terms[::plot_step])) * plot_step\n",
    "    fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "    ax1.plot(x_axis, kl_terms[::plot_step], label=\"KL term value\")\n",
    "    ax1.set_xlabel(\"iteration\")\n",
    "    ax1.set_ylabel(\"KL term\")\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x_axis, kl_weights[::plot_step], color=\"orange\", label=\"KL weight\")\n",
    "    ax2.set_ylabel(\"KL weight\")\n",
    "    ax2.set_ylim(0,1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_losses(total_loss, ce_loss, kl_loss):\n",
    "    plot_step = 10\n",
    "    x_axis = np.arange(len(total_loss[::plot_step])) * plot_step\n",
    "    total_loss = np.array(total_loss[::plot_step])\n",
    "    kl_loss = np.array(kl_loss[::plot_step])\n",
    "    fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "    plt.fill_between(x_axis, np.zeros(len(x_axis)), total_loss, label=\"total loss\")\n",
    "    plt.fill_between(x_axis, np.zeros(len(x_axis)), kl_loss, label=\"kl loss\")\n",
    "    plt.xlabel(\"loss\")\n",
    "    plt.ylabel(\"iteration\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BvL2RqG8bZMd"
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y04SeMISbZMe"
   },
   "outputs": [],
   "source": [
    "def train(vae, inputs, targets, validation_inputs, validation_targets, epochs, vocab_size, hidden_size, \n",
    "          latent_size, max_sentence_length, num_layers=1, learning_rate=0.001,\n",
    "          synthetic=False, input_lens=None, val_input_lens=None,               # text-related parameters\n",
    "          dropout_rate=0.0, unk_index = None,                                  # word dropout parameters\n",
    "          plot=False, plot_lim=1.5, step=1.0, tracked_inputs=None, tracked_targets=None,      # plotting\n",
    "          annealing_args=None, is_aggressive=False, verbose=True):\n",
    "    \n",
    "    opt_dict = {\"not_improved\": 0, \"lr\": learning_rate, \"best_loss\": 1e4}\n",
    "    \n",
    "    decay_epoch = 2\n",
    "    lr_decay = 0.5\n",
    "    max_decay = 5\n",
    "\n",
    "#     enc_optimizer = torch.optim.SGD(vae.encoder.parameters(), lr=learning_rate)\n",
    "#     stoch_enc_optimizer = torch.optim.SGD(vae.stochastic_encoder.parameters(), lr=learning_rate)\n",
    "#     stoch_dec_optimizer = torch.optim.SGD(vae.stochastic_decoder.parameters(), lr=learning_rate)\n",
    "#     dec_optimizer = torch.optim.SGD(vae.decoder.parameters(), lr=learning_rate)\n",
    "    \n",
    "    enc_optimizer = torch.optim.Adam(vae.encoder.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "    stoch_enc_optimizer = torch.optim.Adam(vae.stochastic_encoder.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "    stoch_dec_optimizer = torch.optim.Adam(vae.stochastic_decoder.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "    dec_optimizer = torch.optim.Adam(vae.decoder.parameters(), lr=learning_rate, betas=(0.9, 0.999))\n",
    "    \n",
    "    if annealing_args is not None:\n",
    "        kl_terms = []\n",
    "        kl_weights = []\n",
    "\n",
    "    iteration = decay_cnt = 0\n",
    "    total_losses = []\n",
    "    ce_losses = []\n",
    "    kl_losses = []\n",
    "    total_epoch_losses = []\n",
    "    val_total_epoch_losses = []\n",
    "    \n",
    "    if plot:\n",
    "        plot_mean_space(step, latent_size, vae, tracked_inputs, tracked_targets, lim=plot_lim, iteration=iteration)\n",
    "    \n",
    "    previous_mi = -1\n",
    "        \n",
    "    for epoch in range(epochs):        \n",
    "        for i in np.random.permutation(len(inputs)):\n",
    "            \n",
    "            inner_iter = 1\n",
    "            random_i = i\n",
    "            \n",
    "            burn_num_words = 0\n",
    "            burn_pre_loss = 1e4\n",
    "            burn_cur_loss = 0\n",
    "            while is_aggressive and inner_iter < 100:\n",
    "                x = inputs[random_i]\n",
    "                y = torch.tensor(targets[random_i].reshape(-1), dtype=torch.long)\n",
    "                x_lens = input_lens[random_i] if not synthetic else None\n",
    "                \n",
    "                enc_optimizer.zero_grad()\n",
    "                stoch_enc_optimizer.zero_grad()\n",
    "                stoch_dec_optimizer.zero_grad()\n",
    "                dec_optimizer.zero_grad()\n",
    "                \n",
    "                if synthetic:\n",
    "                    burn_batch_size, burn_sents_len, _ = x.shape\n",
    "                    burn_num_words += burn_sents_len * burn_batch_size\n",
    "                else:\n",
    "                    burn_num_words = np.sum(x_lens)\n",
    "                \n",
    "                mask = None\n",
    "                mean, log_variance, outputs = vae(x, x_lens=x_lens)\n",
    "                if not synthetic:\n",
    "                    mask = (y < padding_index)\n",
    "                    outputs = outputs[mask]\n",
    "                    y = y[mask]\n",
    "    \n",
    "                loss_summary = loss_function(outputs, y, mean, log_variance, max_sentence_length, annealing_args=annealing_args, mask=mask)\n",
    "                \n",
    "                loss = loss_summary[0]\n",
    "                burn_cur_loss += loss.sum().item()\n",
    "                \n",
    "                loss = loss.mean(dim=-1)\n",
    "                loss.backward()\n",
    "                \n",
    "                clip_grad_norm_(vae.parameters(), 5.0)\n",
    "                \n",
    "                stoch_enc_optimizer.step()\n",
    "                enc_optimizer.step()\n",
    "                \n",
    "                random_i = np.random.randint(0, len(inputs)- 1)\n",
    "                if inner_iter % 15 == 0:\n",
    "                    burn_cur_loss = burn_cur_loss / burn_num_words\n",
    "                    if burn_pre_loss - burn_cur_loss < 0:\n",
    "                        break\n",
    "                    burn_pre_loss = burn_cur_loss\n",
    "                    burn_cur_loss = burn_num_words = 0\n",
    "                inner_iter += 1\n",
    "              \n",
    "            x = inputs[i]\n",
    "            y = torch.tensor(targets[i].reshape(-1), dtype=torch.long)\n",
    "            x_lens = input_lens[i] if not synthetic else None  \n",
    "            \n",
    "            mask = None\n",
    "            mean, log_variance, outputs = vae(x, x_lens=x_lens, dropout_rate=dropout_rate, unk_index=unk_index)\n",
    "\n",
    "            if not synthetic:\n",
    "                mask = (y < padding_index)\n",
    "                outputs = outputs[mask]\n",
    "                y = y[mask]\n",
    "            \n",
    "            enc_optimizer.zero_grad()\n",
    "            stoch_enc_optimizer.zero_grad()\n",
    "            stoch_dec_optimizer.zero_grad()\n",
    "            dec_optimizer.zero_grad()\n",
    "            \n",
    "            loss_summary = loss_function(outputs, y, mean, log_variance, max_sentence_length, annealing_args=annealing_args, mask=mask)\n",
    "            \n",
    "            total_losses.append(np.mean(loss_summary[0].data.numpy()))\n",
    "            ce_losses.append(np.mean(loss_summary[1].data.numpy()))\n",
    "            kl_losses.append(np.mean(loss_summary[3].data.numpy()))\n",
    "            \n",
    "            loss = loss_summary[0]\n",
    "                \n",
    "            loss = loss.mean(dim=-1)\n",
    "            \n",
    "            if annealing_args is not None:\n",
    "                kl_terms.append(np.mean(loss_summary[2].data.numpy()))\n",
    "                kl_weights.append(loss_summary[4])     \n",
    "            \n",
    "            loss.backward()\n",
    "            clip_grad_norm_(vae.parameters(), 5.0)\n",
    "            \n",
    "            if not is_aggressive:\n",
    "                stoch_enc_optimizer.step()\n",
    "                enc_optimizer.step()\n",
    "            \n",
    "            dec_optimizer.step()\n",
    "            stoch_dec_optimizer.step()\n",
    "\n",
    "            if (iteration % 100 == 0) and verbose:\n",
    "                print('epoch {} iteration {} loss {:.3f} CE {:.3f} KL {:.3f} weighted KL: {:.3f} weight {:.3f}'.format(epoch+1, \n",
    "                            iteration, loss, loss_summary[1].mean(dim=-1).data.item(), \\\n",
    "                            loss_summary[2].mean(dim=-1).data.item(), \\\n",
    "                            loss_summary[3].mean(dim=-1).data.item(), loss_summary[4]))\n",
    "\n",
    "            iteration += 1\n",
    "            \n",
    "            if annealing_args is not None:\n",
    "                annealing_args['step'] = iteration\n",
    "        \n",
    "        if is_aggressive:\n",
    "            vae.eval()\n",
    "            current_mi = calc_mi(vae, validation_inputs)\n",
    "            vae.train()\n",
    "            print('current_mi:', current_mi)\n",
    "            if current_mi - previous_mi < 0:\n",
    "                is_aggressive = False\n",
    "                print(\"STOP AGGRESSIVE\")\n",
    "\n",
    "            previous_mi = current_mi\n",
    "              \n",
    "        # Validation\n",
    "        vae.eval()\n",
    "        with torch.no_grad():\n",
    "            # NOTE!! if we want to do 100% dropout then we should also add it here.\n",
    "            val_loss, val_kl, val_ppl = test_vae(vae, validation_inputs, validation_targets, val_input_lens, synthetic, annealing_args)\n",
    "            loss, kl, ppl = test_vae(vae, inputs, targets, input_lens, synthetic, annealing_args)\n",
    "            total_epoch_losses.append(loss)\n",
    "            val_total_epoch_losses.append(val_loss)\n",
    "            if verbose:\n",
    "                print ('Epoch [{}/{}], Training Loss: {:.4f},  Training KL: {:.4f}, Training Perplexity: {:5.2f}, Validation Loss: {:.4f}, KL {:.4f}, Val Perplexity: {:5.2f}\\n'\n",
    "                       .format(epoch + 1, epochs, loss, kl, ppl, val_loss, val_kl, val_ppl))\n",
    "            if plot:\n",
    "                plot_mean_space(step, latent_size, vae, tracked_inputs, tracked_targets, lim=plot_lim, iteration=iteration)\n",
    "                \n",
    "            if val_loss > opt_dict[\"best_loss\"]:\n",
    "                opt_dict[\"not_improved\"] += 1\n",
    "                if opt_dict[\"not_improved\"] >= decay_epoch:\n",
    "                    opt_dict[\"best_loss\"] = val_loss\n",
    "                    opt_dict[\"not_improved\"] = 0\n",
    "                    opt_dict[\"lr\"] = opt_dict[\"lr\"] * lr_decay\n",
    "                    #vae.load_state_dict(torch.load(args.save_path))\n",
    "                    print('new lr: %f' % opt_dict[\"lr\"])\n",
    "                    decay_cnt += 1\n",
    "\n",
    "                    enc_optimizer = torch.optim.SGD(vae.encoder.parameters(), lr=opt_dict[\"lr\"])\n",
    "                    stoch_enc_optimizer = torch.optim.SGD(vae.stochastic_encoder.parameters(), lr=opt_dict[\"lr\"])\n",
    "                    stoch_dec_optimizer = torch.optim.SGD(vae.stochastic_decoder.parameters(), lr=opt_dict[\"lr\"])\n",
    "                    dec_optimizer = torch.optim.SGD(vae.decoder.parameters(), lr=opt_dict[\"lr\"])\n",
    "            else:\n",
    "                opt_dict[\"not_improved\"] = 0\n",
    "                opt_dict[\"best_loss\"] = val_loss\n",
    "            \n",
    "            if decay_cnt == max_decay:\n",
    "                break\n",
    "        vae.train()\n",
    "    \n",
    "    if annealing_args is not None and plot:\n",
    "        plot_kl(kl_terms, kl_weights)\n",
    "    if plot:\n",
    "        plot_losses(total_losses, ce_losses, kl_losses)\n",
    "        plt.plot(total_epoch_losses)\n",
    "        plt.plot(val_total_epoch_losses)\n",
    "        plt.show()\n",
    "        \n",
    "def test_vae(model, inputs, targets, input_lens, synthetic=False, annealing_args=None):\n",
    "    kl_loss = ce_loss = 0\n",
    "    num_words = num_sents = 0\n",
    "    for i in np.random.permutation(len(inputs)):\n",
    "        x = inputs[i]\n",
    "        y = torch.tensor(targets[i].reshape(-1), dtype=torch.long)\n",
    "        x_lens = input_lens[i] if not synthetic else None\n",
    "\n",
    "        batch_size, sents_len, _ = x.shape\n",
    "        if synthetic:\n",
    "            num_words += batch_size * sents_len\n",
    "        else:\n",
    "            num_words = np.sum(x_lens)\n",
    "        \n",
    "        num_sents += batch_size\n",
    "        \n",
    "        mask = None\n",
    "        mean, log_variance, outputs = vae(x, x_lens=x_lens)\n",
    "        if not synthetic:\n",
    "            mask = (y < padding_index)\n",
    "            outputs = outputs[mask]\n",
    "            y = y[mask]\n",
    "\n",
    "        loss_summary = loss_function(outputs, y, mean, log_variance, max_sentence_length, annealing_args=annealing_args, mask=mask)\n",
    "\n",
    "        loss_rc = np.sum(loss_summary[1].data.numpy())\n",
    "        loss_kl = np.sum(loss_summary[3].data.numpy())\n",
    "        \n",
    "        ce_loss += loss_rc.item()\n",
    "        kl_loss += loss_kl.item()\n",
    "\n",
    "    #mutual_info = calc_mi(model, test_data_batch)\n",
    "\n",
    "    loss = (kl_loss + ce_loss) / num_sents\n",
    "    kl = kl_loss / num_sents\n",
    "    ppl = np.exp(loss * num_sents / num_words)\n",
    "\n",
    "    return loss, kl, ppl\n",
    "\n",
    "    \n",
    "def calc_mi(model, test_data_batch):\n",
    "    mi = 0\n",
    "    num_examples = 0\n",
    "    for batch_data in test_data_batch:\n",
    "        batch_size = batch_data.shape[0]\n",
    "        num_examples += batch_size\n",
    "        mutual_info = model.calc_mi(batch_data)\n",
    "        mi += mutual_info * batch_size\n",
    "\n",
    "    return mi / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_active_units(vae, train_inputs, train_lengths=None):\n",
    "    all_mus = None\n",
    "    for i in range(len(train_inputs)):\n",
    "        mus, _ = vae.encode(train_inputs[i], train_lengths[i] if train_lengths is not None else None)\n",
    "        mus = mus.squeeze(0)\n",
    "        if all_mus is None:\n",
    "            all_mus = mus.detach().numpy()\n",
    "        else:\n",
    "            all_mus = np.vstack((mus.detach().numpy(), all_mus))\n",
    "    #print(np.sum(np.var(mus.detach().numpy(), axis=0) > 1e-2))\n",
    "    plt.hist(np.var(mus.detach().numpy(), axis=0), bins=50)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pGrpkEVJbZL6"
   },
   "source": [
    "### Load Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-g1xoseTbZL7"
   },
   "outputs": [],
   "source": [
    "test_syn_data = np.loadtxt('synthetic-data/synthetic_test.txt', dtype=int)\n",
    "train_syn_data = np.loadtxt('synthetic-data/synthetic_train.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZfk9HgrbZL-"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# load tracked inputs (for plotting)\n",
    "random_indices = np.random.choice(train_syn_data.shape[0], 500)\n",
    "tracked_inputs = []\n",
    "tracked_targets = []\n",
    "for random_index in random_indices:\n",
    "    tracked_inputs.append(train_syn_data[random_index, :-1])\n",
    "    tracked_targets.append(train_syn_data[random_index, 1:])\n",
    "tracked_inputs = np.expand_dims(np.array(tracked_inputs), axis=-1)\n",
    "tracked_targets = np.expand_dims(np.array(tracked_targets), axis=-1)\n",
    "\n",
    "# load data into batches\n",
    "inputs, targets = get_batches_synthetic(train_syn_data, batch_size)\n",
    "val_inputs, val_targets = get_batches_synthetic(test_syn_data, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m6IIuzUnbZMh"
   },
   "source": [
    "### Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Hzwy-xKbZMi",
    "outputId": "a9700e02-3f3d-4483-8b56-25bb821067e3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# without annealing\n",
    "vocab_size = 1000\n",
    "hidden_size = 50\n",
    "embedding_size = 50\n",
    "latent_size = 1\n",
    "num_layers = 1\n",
    "step = 0.25\n",
    "learning_rate = 1.0\n",
    "epochs = 30\n",
    "max_sentence_length = 10\n",
    "\n",
    "embedding_weights = nn.Embedding(vocab_size, embedding_size).weight\n",
    "vae = VAE(hidden_size, num_layers, embedding_weights, latent_size, synthetic=True)\n",
    "\n",
    "train(vae, inputs, targets, val_inputs, val_inputs, epochs, vocab_size, hidden_size, latent_size, max_sentence_length, plot=True, learning_rate=learning_rate,\n",
    "      synthetic=True, step=step, tracked_inputs=tracked_inputs, tracked_targets=tracked_targets, plot_lim=1.5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_active_units(vae, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSToDB3RbZMl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with annealing\n",
    "vocab_size = 1000\n",
    "hidden_size = 50\n",
    "embedding_size = 50\n",
    "latent_size = 1\n",
    "num_layers = 1\n",
    "step = 0.25\n",
    "learning_rate = 1.0\n",
    "epochs = 10\n",
    "max_sentence_length = 10\n",
    "\n",
    "embedding_weights = nn.Embedding(vocab_size, embedding_size).weight\n",
    "vae = VAE(hidden_size, num_layers, embedding_weights, latent_size, synthetic=True)\n",
    "\n",
    "annealing_args = {'type':'logistic', 'step':0, 'k':0.0025, 'first_step':2500}\n",
    "\n",
    "train(vae, inputs, targets, val_inputs, val_targets, epochs, vocab_size, hidden_size, latent_size, max_sentence_length, plot=True, learning_rate=learning_rate,\n",
    "      synthetic=True, step=step, plot_lim=3, annealing_args=annealing_args, tracked_inputs=tracked_inputs, \n",
    "      tracked_targets=tracked_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_active_units(vae, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6038
    },
    "colab_type": "code",
    "id": "RvITCzNRbZMn",
    "outputId": "29295924-4ea6-47a2-b5cb-ecc92f22c8d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# aggresive training\n",
    "vocab_size = 1000\n",
    "hidden_size = 50\n",
    "embedding_size = 50\n",
    "latent_size = 1\n",
    "num_layers = 1\n",
    "step = 0.25\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "max_sentence_length = 10\n",
    "\n",
    "embedding_weights = nn.Embedding(vocab_size, embedding_size).weight\n",
    "vae = VAE(hidden_size, num_layers, embedding_weights, latent_size, synthetic=True)\n",
    "\n",
    "train(vae, inputs, targets, val_inputs, val_targets, epochs, vocab_size, hidden_size, latent_size, max_sentence_length, plot=True, learning_rate=learning_rate,\n",
    "      synthetic=True, step=step, tracked_inputs=tracked_inputs, tracked_targets=tracked_targets, plot_lim=1.5, is_aggressive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_active_units(vae, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oi_1kQ53bZMq"
   },
   "source": [
    "# TEXT DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNvRgoahbZL3"
   },
   "source": [
    "### Load Penn Treebank data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WK_Yh_Y9bZL4"
   },
   "outputs": [],
   "source": [
    "max_sentence_length = 50\n",
    "train_data, train_data_padded = load_data(\"data/ptb.train.txt\", max_sentence_length)\n",
    "val_data, val_data_padded = load_data(\"data/ptb.valid.txt\", max_sentence_length)\n",
    "test_data, test_data_padded = load_data(\"data/ptb.test.txt\", max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "qAlIrWZTbZMu",
    "outputId": "abd74738-34f0-4baf-da5d-ffc620d73d36"
   },
   "outputs": [],
   "source": [
    "embedding_size = 500\n",
    "epochs_w2v = 100\n",
    "\n",
    "word2vec_model = Word2Vec(train_data, min_count=1, size=embedding_size, window=5)\n",
    "word2vec_model.train(train_data, epochs=epochs_w2v, total_examples=word2vec_model.corpus_count)\n",
    "\n",
    "word2vec_model = Word2Vec.load(\"word2vec.model\")\n",
    "# print(word2vec_model.wv.most_similar(\"stocks\"))\n",
    "# word2vec_model.wv['credit']\n",
    "\n",
    "vocabulary_size = len(word2vec_model.wv.vocab)\n",
    "# print(\"size of the vocabulary:\", vocabulary_size)\n",
    "# word2vec_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "qAlIrWZTbZMu",
    "outputId": "abd74738-34f0-4baf-da5d-ffc620d73d36"
   },
   "outputs": [],
   "source": [
    "# make the word embeddings into a pythorch tensor\n",
    "embedding_weights = word2vec_model.wv.vectors\n",
    "embedding_weights = np.vstack((embedding_weights, np.zeros((1,embedding_size))))  # add zero vector for <pad>\n",
    "embedding_weights = torch.tensor(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "use_first_k = 500\n",
    "padding_index = vocabulary_size\n",
    "train_batches, train_targets, train_sentence_lens = get_batches_text(train_data[:use_first_k], train_data_padded[:use_first_k], \n",
    "                                                                batch_size, padding_index, word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with annealing\n",
    "vocab_size = 1000\n",
    "hidden_size = 50\n",
    "embedding_size = 50\n",
    "latent_size = 2\n",
    "num_layers = 1\n",
    "step = 0.25\n",
    "learning_rate = 0.01\n",
    "epochs = 1\n",
    "\n",
    "vae = VAE(hidden_size, num_layers, embedding_weights, latent_size, synthetic=False)\n",
    "\n",
    "annealing_args = {'type':'logistic', 'step':0, 'k':0.0025, 'first_step':2500}\n",
    "\n",
    "train(vae, inputs, targets, val_inputs, val_targets, epochs, vocab_size, hidden_size, latent_size, max_sentence_length, plot=False, learning_rate=learning_rate,\n",
    "      synthetic=True, step=step, plot_lim=3, annealing_args=annealing_args, tracked_inputs=tracked_inputs, \n",
    "      tracked_targets=tracked_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference/Generation/Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(vae, z, first_word_index, last_word_index, max_sentence_length):\n",
    "    generator_batch_size = z.shape[0]\n",
    "    hidden_concatenated = vae.stochastic_decoder.forward(z)\n",
    "    hidden = torch.split(hidden_concatenated, vae.hidden_dim, dim=-1)\n",
    "    generated_sequence = greedy(vae, hidden, first_word_index, last_word_index, max_sentence_length, generator_batch_size)\n",
    "    return generated_sequence\n",
    "\n",
    "def greedy(vae, hidden, first_word_index, last_word_index, max_sentence_length, generator_batch_size):\n",
    "    vae_decoder = vae.decoder   \n",
    "    first_word_index = torch.tensor(first_word_index, dtype=torch.long)\n",
    "    predicted = []\n",
    "    curr_words = torch.tensor([first_word_index] * generator_batch_size, dtype=torch.long).unsqueeze(1)\n",
    "    iteration = 0\n",
    "    while iteration != max_sentence_length: #next_word_index != last_word_index and\n",
    "        curr_words_embeddings = vae_decoder.embed(curr_words)\n",
    "        curr_words_embeddings = curr_words_embeddings.view(generator_batch_size, 1, vae_decoder.embedding_size)\n",
    "        \n",
    "        outputs, hidden = vae_decoder.lstm(curr_words_embeddings.float(), (hidden[0].unsqueeze(0), hidden[1].unsqueeze(0)))\n",
    "        outputs = vae_decoder.linear(outputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        hidden = (hidden[0].squeeze(0), hidden[1].squeeze(0))\n",
    "        softmax_outputs = F.softmax(outputs, dim=1).detach().numpy()\n",
    "    \n",
    "        next_words_indices = []\n",
    "        for t in range(generator_batch_size):\n",
    "            \n",
    "            # IS THE NEXT LINE OK? DO INDICES MATCH PROBABILITES?            \n",
    "            \n",
    "            next_words_indices.append(np.random.choice(np.arange(softmax_outputs.shape[1]), size=1, p=softmax_outputs[t])[0])        \n",
    "        curr_words = torch.tensor(next_words_indices, dtype=torch.long).unsqueeze(1)\n",
    "                \n",
    "        predicted.append(next_words_indices)\n",
    "\n",
    "        iteration += 1\n",
    "        \n",
    "#     if predicted[-1] != last_word_index:\n",
    "#         predicted.append(last_word_index)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z1 = torch.randn([latent_size]).numpy()\n",
    "z2 = torch.randn([latent_size]).numpy()\n",
    "number_of_sentences_to_decode = 2\n",
    "max_sentence_length = 10\n",
    "interpolation_points = generate_interpolation_sequence(z1, z2, number_of_sentences_to_decode=number_of_sentences_to_decode)\n",
    "\n",
    "sentences = generate(vae, torch.tensor(interpolation_points, dtype=torch.float), word2vec_model.wv.vocab['<sos>'].index, word2vec_model.wv.vocab['<eos>'].index, max_sentence_length)\n",
    "words_sentences = []\n",
    "sentences = np.array(sentences).T\n",
    "for curr_sentence in sentences:\n",
    "    words_curr_sentence = []\n",
    "    for j in curr_sentence:\n",
    "        words_curr_sentence.append(word2vec_model.wv.index2word[j])\n",
    "    words_sentences.append(words_curr_sentence)\n",
    "for s in words_sentences:\n",
    "    for word in s:\n",
    "        print(word, end = ' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VFd2wVP_bZMq"
   },
   "source": [
    "### Create Word2Vec word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P7x5P4wJbZMw"
   },
   "source": [
    "### Define RNNLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sezQbDZvbZMw"
   },
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_size, hidden_size, num_layers, embedding_weights):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(embedding_weights)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocabulary_size)\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "    def forward(self, x, hidden, x_lens, train=True):\n",
    "        batch_size, max_len, _ = x.shape\n",
    "        embedding_dim = self.embedding_size\n",
    "\n",
    "        x = self.embed(torch.tensor(x, dtype=torch.long)).view(batch_size, max_len, embedding_dim)\n",
    "        if train:\n",
    "            x_lens = torch.tensor(x_lens, dtype=torch.long)\n",
    "            x = pack_padded_sequence(x, x_lens, batch_first=True)\n",
    "\n",
    "        out, hidden = self.lstm(x.float(), hidden) \n",
    "        \n",
    "        if train:\n",
    "            out, output_lens = pad_packed_sequence(out, batch_first=True, total_length=max_sentence_length-1)\n",
    "\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3b909vPhbZMy"
   },
   "source": [
    "### Train and predict with RNNLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1689
    },
    "colab_type": "code",
    "id": "DNAqnNW2bZMz",
    "outputId": "e8b5de4f-81c2-4f4c-9caa-caf4be8b18d3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_size = 100\n",
    "output_size = 100\n",
    "hidden_size = 50\n",
    "\n",
    "batch_size = 20\n",
    "use_first_k = 500\n",
    "padding_index = vocabulary_size\n",
    "train_batches, train_targets, train_sentence_lens = get_batches_text(train_data[:use_first_k], train_data_padded[:use_first_k], \n",
    "                                                                batch_size, padding_index, word2vec_model)\n",
    "\n",
    "# make the word embeddings into a pythorch tensor\n",
    "embedding_weights = word2vec_model.wv.vectors\n",
    "embedding_weights = np.vstack((embedding_weights, np.zeros((1,embedding_size))))  # add zero vector for <pad>\n",
    "embedding_weights = torch.tensor(embedding_weights)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_layers = 1\n",
    "epochs = 100\n",
    "\n",
    "model = RNNLM(vocabulary_size, embedding_size, hidden_size, num_layers, embedding_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    hidden = (torch.zeros(num_layers, batch_size, hidden_size), torch.zeros(num_layers, batch_size, hidden_size))\n",
    "    for i in range(len(train_batches)):\n",
    "        x = train_batches[i]\n",
    "        x_lens = train_sentence_lens[i]\n",
    "        y = torch.tensor(train_targets[i].reshape(-1), dtype=torch.long)   \n",
    "        h, c = hidden\n",
    "        h = h.detach()\n",
    "        c = c.detach()\n",
    "        hidden = (h, c)\n",
    "    \n",
    "        outputs, hidden = model(x, hidden, x_lens)\n",
    "        \n",
    "        mask = (y < padding_index)\n",
    "        loss = nn.CrossEntropyLoss()(outputs[mask], y[mask])\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "               .format(epoch + 1, epochs, loss.item(), np.exp(loss.item())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "oaQPc6BXbZM1",
    "outputId": "e2442573-b581-4f04-96bf-5621aaa340ca"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "test_sentences = [\"in terms\"]\n",
    "sentence, _ = tokenize_sentence(test_sentences[0], max_sentence_length)\n",
    "sentence = sentence[:-1]\n",
    "word_indexes = np.array([word2vec_model.wv.vocab[word].index for word in sentence]).reshape(1, len(sentence), 1)\n",
    "\n",
    "hidden = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
    "h, c = hidden\n",
    "h = h.detach()\n",
    "c = c.detach()\n",
    "hidden = (h, c)\n",
    "\n",
    "outputs, hidden = model(word_indexes, hidden, x_lens, train=False)\n",
    "softmax_outputs = F.softmax(outputs, dim=1).detach().numpy()\n",
    "last_word = softmax_outputs[-1,:]\n",
    "\n",
    "predicted_next_word_idx = np.random.choice(range(len(last_word)), p=last_word)\n",
    "print(\"Argmax: \", word2vec_model.wv.index2word[np.argmax(last_word)])\n",
    "print(\"Next word: \", word2vec_model.wv.index2word[predicted_next_word_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BliAoNIsiUr"
   },
   "source": [
    "### Yelp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yelp_data = []\n",
    "# with open('yelp.csv') as csvfile:\n",
    "#     spam_reader = csv.reader(csvfile)\n",
    "#     for i, row in enumerate(spam_reader):\n",
    "#         sentences = sent_tokenize(row[0])\n",
    "#         for sentence in sentences:\n",
    "#             yelp_data.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number_of_sentences = len(yelp_data)\n",
    "# training_data_len = int(0.8 * number_of_sentences)\n",
    "\n",
    "# random_train_indices = np.random.choice(len(yelp_data), training_data_len, replace=False)\n",
    "# yelp_data_train = [yelp_data[i] for i in random_train_indices]\n",
    "\n",
    "# all_indices = list(range(number_of_sentences))\n",
    "# validation_indices = [i for i in all_indices if i not in random_train_indices]\n",
    "# yelp_data_validation = [yelp_data[i] for i in validation_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yelp_original_train, yelp_padded_train, yelp_original_validation, yelp_padded_validation = [], [], [], []\n",
    "# max_sentence_len = 50\n",
    "# occurrences_train = {}\n",
    "# for line in yelp_data_train:\n",
    "#     sentence, padded_sentence, occurrences_train = tokenize_sentence(line, max_sentence_len, occurrences_train)\n",
    "#     yelp_original_train.append(sentence)\n",
    "#     yelp_padded_train.append(padded_sentence)\n",
    "    \n",
    "# for line in yelp_data_validation:\n",
    "#     sentence, padded_sentence = tokenize_sentence(line, max_sentence_len)\n",
    "#     yelp_original_validation.append(sentence)\n",
    "#     yelp_padded_validation.append(padded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(occurrences_train[\"cox\"])\n",
    "# for i, sent in enumerate(yelp_original_train):\n",
    "#     for j, w in enumerate(sent):\n",
    "#         if w != '<unk>' and occurrences_train[w] <= 2:\n",
    "#              yelp_original_train[i][j] = '<unk>'\n",
    "# # print(yelp_original_train[498])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_size = 512\n",
    "# epochs_w2v = 100\n",
    "\n",
    "# word2vec_yelp = Word2Vec.load(\"word2vec_yelp.model\")\n",
    "\n",
    "# # word2vec_yelp = Word2Vec(yelp_original_train, min_count=1, size=embedding_size, window=5)\n",
    "# # word2vec_yelp.train(yelp_original_train, epochs=epochs_w2v, total_examples=word2vec_yelp.corpus_count)\n",
    "\n",
    "# # word2vec_yelp.save(\"word2vec_yelp.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make the word embeddings into a pythorch tensor\n",
    "# embedding_weights = word2vec_yelp.wv.vectors\n",
    "# embedding_weights = np.vstack((embedding_weights, np.zeros((1,embedding_size))))  # add zero vector for <pad>\n",
    "# embedding_weights = torch.tensor(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(word2vec_yelp.wv.most_similar(\"coffee\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 16\n",
    "# vocabulary_size = len(word2vec_yelp.wv.vocab)\n",
    "# padding_index = vocabulary_size\n",
    "\n",
    "# yelp_train_inputs, yelp_train_targets, yelp_train_lengths = \\\n",
    "#                 get_batches_text(yelp_original_train, yelp_padded_train, batch_size, padding_index, word2vec_yelp)\n",
    "\n",
    "# yelp_val_inputs, yelp_val_targets, yelp_val_lengths = \\\n",
    "#                 get_batches_text(yelp_original_validation, yelp_padded_validation, batch_size, padding_index, word2vec_yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 1024\n",
    "# latent_size = 1\n",
    "# num_layers = 1\n",
    "# step = 0.25\n",
    "# learning_rate = 0.01\n",
    "# epochs = 5\n",
    "# max_sentence_length = 50\n",
    "\n",
    "# vae = VAE(hidden_size, num_layers, embedding_weights, latent_size, synthetic=True)\n",
    "\n",
    "# train(vae, yelp_train_inputs[:10], yelp_train_targets[:10], yelp_val_inputs, epochs, vocabulary_size, \n",
    "#       hidden_size, latent_size, max_sentence_length, yelp_train_lengths[:10], plot=False, learning_rate=learning_rate,\n",
    "#       synthetic=False, step=step, tracked_inputs=None, tracked_targets=None, plot_lim=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_len = 50\n",
    "yelp_train_data_original, yelp_train_data_padded = load_data(\"yelp_data/yelp.train.txt\", max_sentence_len, with_labels=True)\n",
    "yelp_test_data_original, yelp_test_data_padded = load_data(\"yelp_data/yelp.test.txt\", max_sentence_len, with_labels=True)\n",
    "yelp_val_dat_original, yelp_val_data_padded = load_data(\"yelp_data/yelp.valid.txt\", max_sentence_len, with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "epochs_w2v = 100\n",
    "word2vec_model_name = \"word2vec_yelp.model\"\n",
    "\n",
    "if os.path.isfile(word2vec_model_name):\n",
    "    print('Loading word2vec model:', word2vec_model_name)\n",
    "    word2vec_yelp = Word2Vec.load(word2vec_model_name)\n",
    "else:\n",
    "    print('Training word2vec model')\n",
    "    word2vec_yelp = Word2Vec(yelp_train_data_original, min_count=1, size=embedding_size, window=5)\n",
    "    word2vec_yelp.train(yelp_train_data_original, epochs=epochs_w2v, total_examples=word2vec_yelp.corpus_count)\n",
    "    word2vec_yelp.save(word2vec_model_name)\n",
    "    print('Saved word2vec model:', word2vec_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the word embeddings into a pythorch tensor\n",
    "embedding_weights = word2vec_yelp.wv.vectors\n",
    "embedding_weights = np.vstack((embedding_weights, np.zeros((1,embedding_size))))  # add zero vector for <pad>\n",
    "embedding_weights = torch.tensor(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2vec_yelp.wv.most_similar(\"coffee\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "vocabulary_size = len(word2vec_yelp.wv.vocab)\n",
    "padding_index = vocabulary_size\n",
    "\n",
    "yelp_train_inputs, yelp_train_targets, yelp_train_lengths = \\\n",
    "                get_batches_text(yelp_train_data_original, yelp_train_data_padded, batch_size, padding_index, word2vec_yelp, '_unk')\n",
    "\n",
    "yelp_test_inputs, yelp_test_targets, yelp_test_lengths = \\\n",
    "                get_batches_text(yelp_test_data_original, yelp_test_data_padded, batch_size, padding_index, word2vec_yelp, '_unk')\n",
    "\n",
    "yelp_val_inputs, yelp_val_targets, yelp_val_lengths = \\\n",
    "                get_batches_text(yelp_val_dat_original, yelp_val_data_padded, batch_size, padding_index, word2vec_yelp, '_unk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1024\n",
    "latent_size = 1\n",
    "num_layers = 1\n",
    "step = 0.25\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "\n",
    "vae = VAE(hidden_size, num_layers, embedding_weights, latent_size, synthetic=True)\n",
    "\n",
    "train(vae, yelp_train_inputs[:], yelp_train_targets[:], yelp_val_inputs, yelp_val_targets, epochs, vocabulary_size, \n",
    "      hidden_size, latent_size, max_sentence_length, yelp_train_lengths[:], plot=False, learning_rate=learning_rate,\n",
    "      synthetic=False, step=step, tracked_inputs=None, tracked_targets=None, plot_lim=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENCODER-DECODER BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    # just a basic encoder-decoder with no VAE layers in the middle\n",
    "    def __init__(self, hidden_dim, num_layers, embedding_weights, synthetic=False):\n",
    "        super(Baseline, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_first = True\n",
    "        \n",
    "        self.encoder = Encoder(self.hidden_dim, num_layers, embedding_weights, synthetic)\n",
    "        self.decoder = Decoder(self.hidden_dim, num_layers, embedding_weights, synthetic)\n",
    "\n",
    "        if synthetic:          \n",
    "            for param in self.parameters():\n",
    "                nn.init.uniform_(param, -0.01, 0.01)\n",
    "            nn.init.uniform_(self.encoder.embed.weight, -0.1, 0.1)\n",
    "            nn.init.uniform_(self.decoder.embed.weight, -0.1, 0.1)\n",
    "        \n",
    "    def encode(self, x, x_lens=None):\n",
    "        batch_size, max_len, _ = x.shape\n",
    "        hidden = self.encoder.init_hidden(batch_size)\n",
    "        _, hidden = self.encoder.forward(x, hidden, x_lens)\n",
    "        return hidden\n",
    "        \n",
    "    def decode(self, hidden, x, x_lens=None, train=True): \n",
    "        outputs, _ = self.decoder.forward(x, hidden, x_lens, train)\n",
    "        return outputs\n",
    "    \n",
    "    def forward(self, x, x_lens=None):\n",
    "        # the last hidden state of the encoder is the first hidden state of the decoder\n",
    "        hidden = self.encode(x, x_lens)\n",
    "#         hidden_concatenated = self.stochastic_decoder.forward(z)\n",
    "#         hidden = torch.split(hidden_concatenated, self.hidden_dim, dim=2)\n",
    "        outputs = self.decode(hidden, x, x_lens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_loss_function(outputs, labels, seq_length, mask=None):\n",
    "    if mask is not None:\n",
    "        BCE = torch.zeros(mean.shape[1] * (seq_length - 1))\n",
    "        BCE[mask] = nn.CrossEntropyLoss(reduction='none')(outputs, labels)\n",
    "    else:\n",
    "        BCE = nn.CrossEntropyLoss(reduction='none')(outputs, labels)\n",
    "    BCE = BCE.view(mean.shape[1], -1).sum(-1)\n",
    "    return BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_baseline(model, inputs, targets, input_lens, synthetic=False):\n",
    "    total_loss = 0\n",
    "    num_words = num_sents = 0\n",
    "    for i in np.random.permutation(len(inputs)):\n",
    "        x = inputs[i]\n",
    "        y = torch.tensor(targets[i].reshape(-1), dtype=torch.long)\n",
    "        x_lens = input_lens[i] if not synthetic else None\n",
    "\n",
    "        batch_size, sents_len, _ = x.shape\n",
    "        if synthetic:\n",
    "            num_words += batch_size * sents_len\n",
    "        else:\n",
    "            num_words = np.sum(x_lens)\n",
    "        num_sents += batch_size\n",
    "        \n",
    "        mask = None\n",
    "        outputs = model(x, x_lens=x_lens)\n",
    "        if not synthetic:\n",
    "            mask = (y < padding_index)\n",
    "            outputs = outputs[mask]\n",
    "            y = y[mask]\n",
    "        curr_loss = baseline_loss_function(outputs, y, max_sentence_length, mask=mask)\n",
    "        curr_loss = np.sum(curr_loss.data.numpy())\n",
    "        total_loss += curr_loss.item()\n",
    "    \n",
    "    total_loss /= num_sents\n",
    "    ppl = np.exp(total_loss * num_sents / num_words)\n",
    "    return total_loss, ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_baseline(model, inputs, targets, val_inputs, val_targets, epochs, vocab_size, hidden_size, \n",
    "                   max_sentence_length, input_lens=None, val_input_lens=None, synthetic=False, \n",
    "                   num_layers=1, learning_rate=0.001, verbose_level=1, plot=False):\n",
    "\n",
    "    opt_dict = {\"not_improved\": 0, \"lr\": learning_rate, \"best_loss\": 1e4}\n",
    "    decay_epoch = 2\n",
    "    lr_decay = 0.5\n",
    "    max_decay = 5\n",
    "    decay_cnt = 0\n",
    "    \n",
    "    enc_optimizer = torch.optim.SGD(model.encoder.parameters(), lr=learning_rate)\n",
    "    dec_optimizer = torch.optim.SGD(model.decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    iteration = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "            \n",
    "    for epoch in range(epochs):        \n",
    "        for i in np.random.permutation(len(inputs)):\n",
    "            x = inputs[i]\n",
    "            y = torch.tensor(targets[i].reshape(-1), dtype=torch.long)\n",
    "            x_lens = input_lens[i] if not synthetic else None \n",
    "            \n",
    "            mask = None\n",
    "            # do the forward pass\n",
    "            outputs = model(x, x_lens=x_lens)\n",
    "\n",
    "            if not synthetic:\n",
    "                mask = (y < padding_index)\n",
    "                outputs = outputs[mask]\n",
    "                y = y[mask]\n",
    "            \n",
    "            enc_optimizer.zero_grad()\n",
    "            dec_optimizer.zero_grad()\n",
    "            \n",
    "            # compute the cross entropy loss\n",
    "            loss = baseline_loss_function(outputs, y, max_sentence_length, mask=mask)                \n",
    "            loss = loss.mean(dim=-1) # take the mean (same as divide by batch size?)\n",
    "            \n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), 5.0)\n",
    "            enc_optimizer.step()\n",
    "            dec_optimizer.step()\n",
    "            \n",
    "            if (iteration % 100 == 0) and (verbose_level == 2):\n",
    "                print('epoch {}\\titeration {}\\ttraining loss {:.3f}'.format(epoch+1, iteration, loss))\n",
    "\n",
    "            iteration += 1\n",
    "              \n",
    "        # evaluate on the validation data\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_loss, train_ppl = test_baseline(model, inputs, targets, input_lens, synthetic)\n",
    "            val_loss, val_ppl = test_baseline(model, val_inputs, val_targets, val_input_lens, synthetic)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            if verbose_level > 0:\n",
    "                print ('Epoch [{}/{}], Training Loss: {:.4f} Perplexity: {:.4f}, Validation Loss: {:.4f} Perplexity {:.4f}'\n",
    "                       .format(epoch+1, epochs, train_loss, train_ppl, val_loss, val_ppl))\n",
    "\n",
    "            # are we still decaying with the same logic?\n",
    "            if val_loss > opt_dict[\"best_loss\"]:\n",
    "                opt_dict[\"not_improved\"] += 1\n",
    "                if opt_dict[\"not_improved\"] >= decay_epoch:\n",
    "                    opt_dict[\"best_loss\"] = val_loss\n",
    "                    opt_dict[\"not_improved\"] = 0\n",
    "                    opt_dict[\"lr\"] = opt_dict[\"lr\"] * lr_decay\n",
    "                    #vae.load_state_dict(torch.load(args.save_path))\n",
    "                    print('new lr: %f' % opt_dict[\"lr\"])\n",
    "                    decay_cnt += 1\n",
    "\n",
    "                    enc_optimizer = torch.optim.SGD(model.encoder.parameters(), lr=opt_dict[\"lr\"])\n",
    "                    dec_optimizer = torch.optim.SGD(model.decoder.parameters(), lr=opt_dict[\"lr\"])\n",
    "            else:\n",
    "                opt_dict[\"not_improved\"] = 0\n",
    "                opt_dict[\"best_loss\"] = val_loss\n",
    "            \n",
    "            if decay_cnt == max_decay:\n",
    "                break\n",
    "        model.train()\n",
    "    \n",
    "    if plot:\n",
    "        plt.plot(train_losses)\n",
    "        plt.plot(val_losses)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without annealing\n",
    "vocab_size = 1000\n",
    "hidden_size = 50\n",
    "embedding_size = 50\n",
    "num_layers = 1\n",
    "learning_rate = 1.0\n",
    "epochs = 20\n",
    "max_sentence_length = 10\n",
    "\n",
    "embedding_weights = nn.Embedding(vocab_size, embedding_size).weight\n",
    "baseline_model = Baseline(hidden_size, num_layers, embedding_weights, synthetic=True)\n",
    "\n",
    "# verbose level 0 - print nothing\n",
    "# verbose level 1 - print only at the end of each epoch\n",
    "# verbose level 2 - print everything\n",
    "verbose_level = 1\n",
    "\n",
    "train_baseline(baseline_model, inputs, targets, val_inputs, val_targets, epochs, vocab_size, hidden_size, max_sentence_length, \n",
    "               learning_rate=learning_rate, synthetic=True, verbose_level=verbose_level, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sentence_length = 50\n",
    "train_data, train_data_padded = load_data(\"data/ptb.train.txt\", max_sentence_length)\n",
    "val_data, val_data_padded = load_data(\"data/ptb.valid.txt\", max_sentence_length)\n",
    "test_data, test_data_padded = load_data(\"data/ptb.test.txt\", max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 512\n",
    "epochs_w2v = 100\n",
    "\n",
    "word2vec_model = Word2Vec(train_data, min_count=1, size=embedding_size, window=5)\n",
    "word2vec_model.train(train_data, epochs=epochs_w2v, total_examples=word2vec_model.corpus_count)\n",
    "\n",
    "# word2vec_model = Word2Vec.load(\"word2vec.model\")\n",
    "# print(word2vec_model.wv.most_similar(\"stocks\"))\n",
    "# word2vec_model.wv['credit']\n",
    "\n",
    "vocabulary_size = len(word2vec_model.wv.vocab)\n",
    "# print(\"size of the vocabulary:\", vocabulary_size)\n",
    "# word2vec_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the word embeddings into a pythorch tensor\n",
    "embedding_weights = word2vec_model.wv.vectors\n",
    "embedding_weights = np.vstack((embedding_weights, np.zeros((1,embedding_size))))  # add zero vector for <pad>\n",
    "embedding_weights = torch.tensor(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# use_first_k = 500\n",
    "padding_index = vocabulary_size\n",
    "train_batches, train_targets, train_sentence_lens = get_batches_text(train_data, train_data_padded, batch_size, padding_index, word2vec_model)\n",
    "val_batches, val_targets, val_sentence_lens = get_batches_text(val_data, val_data_padded, batch_size, padding_index, word2vec_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1024\n",
    "latent_size = 13\n",
    "num_layers = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "# dropout rate 80%\n",
    "unk_index = word2vec_model.wv.vocab[\"<unk>\"].index\n",
    "dropout_prob = 0.8\n",
    "\n",
    "vae = VAE(hidden_size, num_layers, embedding_weights, latent_size, synthetic=False)\n",
    "\n",
    "train(vae, train_batches, train_targets, val_batches, val_targets, epochs, vocab_size, hidden_size, latent_size, \n",
    "      max_sentence_length, plot=False, verbose=True, learning_rate=learning_rate, synthetic=False,\n",
    "      input_lens=train_sentence_lens, val_input_lens=val_sentence_lens, unk_index=unk_index, dropout_rate=dropout_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "refactored-workspace-text.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
