{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20bc1cc95f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Mihaela\n",
      "[nltk_data]     Stoycheva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total sentences: 42068\n"
     ]
    }
   ],
   "source": [
    "# tokenizer = TweetTokenizer(preserve_case=False)\n",
    "tok = MWETokenizer([('<', 'unk', '>')], separator = '')\n",
    "\n",
    "train_data = []\n",
    "vocabulary = []\n",
    "\n",
    "with open(\"data/ptb.train.txt\") as f:\n",
    "    total_sentences = 0\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        s = word_tokenize(line)\n",
    "        s = tok.tokenize(s)\n",
    "        s = ['<sos>'] + s + ['<eos>']\n",
    "        total_sentences += 1\n",
    "        train_data.append(s)\n",
    "        vocabulary += s\n",
    "\n",
    "vocabulary = sorted(list(set(vocabulary)))\n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "print(\"total sentences:\", total_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '$', '&', \"'\", \"'80s\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", '.', '10-year', '100-share', '12-month', '12-year', '13-week', '13th', '14-year-old', '190-point', '190.58-point', '1920s', '1930s', '1950s', '1960s', '1970s', '1980s', '1990s', '19th', '1\\\\/2-year', '2-for-1', '20-year', '20th', '24-hour', '26-week', '30-day', '30-share', '30-year', '300-a-share', '300-day', '40-year-old', '45-year-old', '500-stock', '52-week', '<eos>', '<sos>', '<unk>', 'N', '\\\\*', '\\\\*\\\\*', 'a', 'a.', 'a.c.', 'a.g.', 'a.m', 'a.m.', 'a.p', 'ab', 'aba', 'abandon', 'abandoned', 'abandoning', 'abbie', 'abc', 'ability', 'able', 'abm', 'aboard', 'abolish', 'abolished', 'aborted', 'abortion', 'abortion-rights', 'abortions', 'about', 'above', 'abrams', 'abramson', 'abroad', 'abrupt', 'abruptly', 'absence', 'absolutely', 'absorb', 'absorbed', 'absurd', 'abundant', 'abuse', 'abused', 'abuses', 'academic', 'academy', 'acadia', 'accelerate', 'accelerated', 'accelerating', 'acceleration', 'accept', 'acceptable', 'acceptance']\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66890574, 97292600)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model1 = Word2Vec(train_data, min_count=1, size=100, window=5)\n",
    "model1.train(train_data, len(train_data), epochs=100, total_examples=model1.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shares', 0.6124134659767151),\n",
       " ('stocks', 0.5938287973403931),\n",
       " ('equity', 0.5454078912734985),\n",
       " ('share', 0.49382197856903076),\n",
       " ('junk-bond', 0.4811207056045532),\n",
       " ('mercantile', 0.42205220460891724),\n",
       " ('plunge', 0.41612717509269714),\n",
       " ('junk', 0.4154773950576782),\n",
       " ('over-the-counter', 0.4082963168621063),\n",
       " ('options', 0.40443819761276245)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar(\"stock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('f.', 0.430519163608551),\n",
       " ('l.', 0.4049091041088104),\n",
       " ('u.s.a.', 0.4036238193511963),\n",
       " ('s.', 0.3933088183403015),\n",
       " ('w.', 0.3931712806224823),\n",
       " ('h.', 0.3906556963920593),\n",
       " ('t.', 0.3863432705402374),\n",
       " ('m.', 0.38257697224617004),\n",
       " ('r.', 0.3808728754520416),\n",
       " ('jr.', 0.37582874298095703)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loans', 0.4597245454788208),\n",
       " ('debt', 0.4546087682247162),\n",
       " ('loan', 0.4374544322490692),\n",
       " ('financial', 0.3911444842815399),\n",
       " ('payment', 0.3867035508155823),\n",
       " ('capital', 0.3803357779979706),\n",
       " ('financing', 0.3711961507797241),\n",
       " ('citicorp', 0.3703234791755676),\n",
       " ('borrowing', 0.36570313572883606),\n",
       " ('credit-card', 0.36446505784988403)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar(\"credit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.0183839e-01, -8.5344392e-01,  3.2377582e+00, -7.0825732e-01,\n",
       "       -8.6280912e-01,  1.1251957e+00,  1.6312332e+00,  3.1292782e+00,\n",
       "       -1.6751940e+00,  5.7619697e-01, -2.1294656e+00, -3.1530521e+00,\n",
       "       -2.8109512e+00, -1.6537700e+00,  5.8846813e-01,  1.1687769e+00,\n",
       "        1.2105019e+00,  2.8258069e+00, -5.5699086e-01, -8.1041855e-01,\n",
       "       -4.6090314e-01,  2.0775390e+00,  3.2119966e+00, -1.4342258e+00,\n",
       "        1.0262493e+00, -2.8396878e-01, -4.2756662e-01, -2.1494949e+00,\n",
       "       -2.5634935e+00, -7.2201431e-01, -4.5823645e-02,  5.1110852e-01,\n",
       "        4.5512161e-01, -1.9884517e+00,  3.3549728e+00,  2.2448552e+00,\n",
       "        2.7554889e+00, -2.4954386e+00, -2.8861184e+00,  2.2421050e+00,\n",
       "        2.0623786e+00, -1.7116361e+00, -5.9748119e-01, -1.8433223e+00,\n",
       "        1.5207335e-01, -2.9780552e+00, -2.2002859e+00, -6.3473225e-01,\n",
       "        4.2796385e-02,  7.9394716e-01,  2.9584658e-01,  3.0709546e+00,\n",
       "       -1.0523658e+00, -3.1390479e-01,  3.2061848e-01, -2.5827928e+00,\n",
       "       -3.2572970e+00,  2.8355889e+00,  2.8657424e+00,  3.6734507e+00,\n",
       "       -2.0779333e+00, -2.1833487e-01,  2.1293318e+00,  1.7351692e+00,\n",
       "       -1.0096744e+00, -3.9866908e+00, -1.9865298e+00,  1.5489832e+00,\n",
       "        1.0344214e+00,  1.4964536e+00,  3.6207634e-01,  4.5102353e+00,\n",
       "       -2.0929413e+00,  7.1944356e-01,  2.5205464e+00, -3.6433897e+00,\n",
       "        4.9061888e-01, -1.6361811e+00,  1.1433600e+00, -2.5330829e-03,\n",
       "       -3.3071360e-01, -3.0491799e-01, -3.7095931e+00,  3.0474205e+00,\n",
       "       -8.3299333e-01, -7.7771366e-02,  1.7802783e+00,  7.8887999e-01,\n",
       "        9.5140922e-01, -2.7358525e+00, -1.6089020e+00,  3.1412079e+00,\n",
       "        1.6743878e+00, -1.4420104e+00,  8.6359012e-01,  3.2774977e-02,\n",
       "        2.1486490e+00,  2.5253046e+00,  2.6902494e+00, -7.2944486e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv['credit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(vocabulary)\n",
    "input_size = 100\n",
    "output_size = 100\n",
    "hidden_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_word = 'money'\n",
    "embedding_vec = model1.wv[input_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the word embeddings into a pythorch tensor\n",
    "weights = torch.FloatTensor(model1.wv.vectors)\n",
    "\n",
    "# NN MODEL\n",
    "embedded = nn.Embedding.from_pretrained(weights) #input layer\n",
    "lstm = nn.LSTM(input_size, hidden_size, num_layers=1) #lstm layer\n",
    "out = nn.Linear(hidden_size, output_size)\n",
    "softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "# loss_function = nn.NLLLoss()\n",
    "# optimizer = optim.SGD( lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = train_data[2]\n",
    "input_idx = [model1.wv.vocab[x].index for x in input_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word: <sos>\n",
      "predicted_class: president\n",
      "input word: mr.\n",
      "predicted_class: market\n",
      "input word: <unk>\n",
      "predicted_class: he\n",
      "input word: is\n",
      "predicted_class: co.\n",
      "input word: chairman\n",
      "predicted_class: which\n",
      "input word: of\n",
      "predicted_class: co.\n",
      "input word: <unk>\n",
      "predicted_class: president\n",
      "input word: n.v.\n",
      "predicted_class: shares\n",
      "input word: the\n",
      "predicted_class: to\n",
      "input word: dutch\n",
      "predicted_class: last\n",
      "input word: publishing\n",
      "predicted_class: most\n",
      "input word: group\n",
      "predicted_class: only\n",
      "input word: <eos>\n",
      "predicted_class: president\n"
     ]
    }
   ],
   "source": [
    "# intialise first hidden state\n",
    "(hidden, cell) = (torch.zeros(1, 1, hidden_size), torch.zeros(1, 1, hidden_size))\n",
    "\n",
    "for idx in input_idx:\n",
    "    print(\"input word:\", model1.wv.index2word[idx])\n",
    "    output = embedded(torch.tensor([idx], dtype=torch.long)).view(1, 1, 100)\n",
    "    output, (hidden, cell) = lstm(output, (hidden, cell))\n",
    "    output = softmax(out(output[0]))\n",
    "    predicted_idx = np.argmax(output.detach().numpy())\n",
    "    print(\"predicted_class: {0}\".format(model1.wv.index2word[predicted_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn_language_model(nn.Module):\n",
    "    def __init__(self, vocabulary_size, input_size, hidden_size, num_layers, weights):\n",
    "        super(rnn_language_model, self).__init__()\n",
    "        self.embed = nn.Embedding.from_pretrained(weights)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocabulary_size)\n",
    "        \n",
    "    def forward(self, x, h):\n",
    "        x = self.embed(torch.tensor(x, dtype=torch.long)).view(1, len(x), 100)\n",
    "        out, (h, c) = self.lstm(x, h)\n",
    "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
    "        out = self.linear(out)\n",
    "        return out, (h, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "num_layers = 1\n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "\n",
    "model = rnn_language_model(vocabulary_size, input_size, hidden_size, num_layers, weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Fixed batch size - this should probably be changed later since the size will vary from sentence to sentence, not sure how...\n",
    "def get_batch(data, batch_size):\n",
    "    batches = []\n",
    "    count = 0\n",
    "    current_batch = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            if count == batch_size + 1:\n",
    "                batches.append(current_batch)\n",
    "                current_batch = []\n",
    "                count = 0\n",
    "            else:\n",
    "                current_batch.append(data[i][j])\n",
    "                count += 1\n",
    "    return batches  \n",
    "\n",
    "batches = get_batch(train_data[1:100], batch_size)\n",
    "print(len(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Loss: 9.3038, Perplexity: 10979.92\n",
      "Epoch [10/100], Loss: 9.2138, Perplexity: 10035.12\n",
      "Epoch [20/100], Loss: 9.0911, Perplexity: 8875.58\n",
      "Epoch [30/100], Loss: 8.9311, Perplexity: 7563.61\n",
      "Epoch [40/100], Loss: 8.7241, Perplexity: 6149.08\n",
      "Epoch [50/100], Loss: 8.4693, Perplexity: 4766.16\n",
      "Epoch [60/100], Loss: 8.1760, Perplexity: 3554.47\n",
      "Epoch [70/100], Loss: 7.8569, Perplexity: 2583.50\n",
      "Epoch [80/100], Loss: 7.5062, Perplexity: 1819.37\n",
      "Epoch [90/100], Loss: 7.1628, Perplexity: 1290.56\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(epochs):\n",
    "    (hidden, cell) = (torch.zeros(num_layers, batch_size, hidden_size), torch.zeros(num_layers, batch_size, hidden_size))\n",
    "    for i in range(len(batches)):\n",
    "        inputs = [model1.wv.vocab[x].index for x in batches[i][:-1]]\n",
    "        targets = torch.tensor([model1.wv.vocab[x].index for x in batches[i][1:]], dtype=torch.long)\n",
    "\n",
    "        hidden = hidden.detach()\n",
    "        cell = cell.detach()\n",
    "    \n",
    "        outputs, (hidden, cell) = model(inputs, (hidden, cell))\n",
    "        loss = nn.CrossEntropyLoss()(outputs, targets)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
    "               .format(epoch, epochs, loss.item(), np.exp(loss.item())))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
